{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3e4f8e3-7342-4233-8c9d-e0366374978b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1b1569a-f5a2-4583-a153-429d6181215d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-95jfayfddUNwaK7kGMNZG74bZ889S', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I'm sorry, I am not able to provide real-time flight information. I recommend checking with your preferred airline or a flight tracking website for the most up-to-date information on flights from Delhi to Mumbai.\", role='assistant', function_call=None, tool_calls=None))], created=1711154222, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_3bc1b5746c', usage=CompletionUsage(completion_tokens=41, prompt_tokens=19, total_tokens=60))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ee702dd-332b-453d-a40b-64da9c896f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I am not able to provide real-time flight information. I recommend checking with your preferred airline or a flight tracking website for the most up-to-date information on flights from Delhi to Mumbai.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a81879c-afde-4f58-88ae-6e07f0bef4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_descriptions = [\n",
    "    {\n",
    "        \"name\": \"get_flight_info\",\n",
    "        \"description\": \"Get flight information between two locations\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"loc_origin\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The departure airport, e.g. DEL\",\n",
    "                },\n",
    "                \"loc_destination\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The destination airport, e.g. MUM\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"loc_origin\", \"loc_destination\"],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b681de2c-2e99-400d-84e3-a5036a6cde86",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"When's the next flight from new delhi to mumbai?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9f1402d-a846-4a0c-af7b-4f3a4074ce92",
   "metadata": {},
   "outputs": [],
   "source": [
    "response2 = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": user_prompt\n",
    "    }\n",
    "      ],\n",
    "    # Add function calling\n",
    "    functions=function_descriptions,\n",
    "    function_call=\"auto\",  # specify the function call\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "285345a7-5e71-4365-a603-5b54f913ec2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-95jfbfAMCLiz4mrP1evGnnxso12uf', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\"loc_origin\":\"DEL\",\"loc_destination\":\"BOM\"}', name='get_flight_info'), tool_calls=None))], created=1711154223, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_3bc1b5746c', usage=CompletionUsage(completion_tokens=22, prompt_tokens=87, total_tokens=109))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3266c9af-c667-48b2-b59d-cc3e3c2faf97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"loc_origin\":\"DEL\",\"loc_destination\":\"BOM\"}'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response2.choices[0].message.function_call.arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "54cb71e7-8e99-4cdd-bcd9-c02bc78dc015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function simulating a real flight info API\n",
    "\n",
    "from datetime import datetime,timedelta\n",
    "def get_flight_info(loc_origin, loc_destination):\n",
    "    \"\"\"Get flight information between two locations.\"\"\"\n",
    "\n",
    "    # Example output returned from an API or database\n",
    "    flight_info = {\n",
    "        \"loc_origin\": loc_origin,\n",
    "        \"loc_destination\": loc_destination,\n",
    "        \"datetime\": str(datetime.now() + timedelta(hours=2)),\n",
    "        \"airline\": \"KLM\",\n",
    "        \"flight\": \"KL643\",\n",
    "    }\n",
    "\n",
    "    return json.dumps(flight_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "88bead2a-0d94-4b91-b128-1b1c88e369e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "params=json.loads(response2.choices[0].message.function_call.arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ab3dc3c-02a0-4703-9be0-6873dc15c39f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loc_origin': 'DEL', 'loc_destination': 'BOM'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bf253d3b-9766-4720-a9c2-11e7d30d5ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DEL'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(response2.choices[0].message.function_call.arguments).get(\"loc_origin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c632b523-3979-448f-9026-92bb8cff5196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'get_flight_info'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response2.choices[0].message.function_call.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "43665b78-4dd7-4bc9-966d-b45da10eb1f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.get_flight_info(loc_origin, loc_destination)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(response2.choices[0].message.function_call.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5442b897-0d05-444e-90b9-ea68d02b347b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_function=eval(response2.choices[0].message.function_call.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0eb9d604-277f-446e-93c8-10cba66f6462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loc_origin\": \"DEL\", \"loc_destination\": \"BOM\", \"datetime\": \"2024-03-23 02:37:05.530245\", \"airline\": \"KLM\", \"flight\": \"KL643\"}\n"
     ]
    }
   ],
   "source": [
    "flight = chosen_function(**params)\n",
    "\n",
    "print(flight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ff269362-e8b0-4ee6-a112-d89f7baf571d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"When's the next flight from new delhi to mumbai?\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8f8d0597-ad65-4646-85cd-07f1aad2f3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response3 = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "    {\"role\": \"user\",\"content\": user_prompt},\n",
    "    {\"role\": \"function\", \"name\": response2.choices[0].message.function_call.name, \"content\": flight}\n",
    "      ],\n",
    "    # Add function calling\n",
    "    functions=function_descriptions,\n",
    "    function_call=\"auto\",  # specify the function call\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3ccf3de7-dadd-4a9e-b305-409683d7d035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-95jfe9HHwGwCnOZQtkZvoZQOD0mpn', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The next flight from New Delhi (DEL) to Mumbai (BOM) is on March 23, 2024. The flight is operated by KLM with flight number KL643.', role='assistant', function_call=None, tool_calls=None))], created=1711154226, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_3bc1b5746c', usage=CompletionUsage(completion_tokens=39, prompt_tokens=143, total_tokens=182))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "14812e9c-933b-46d7-9834-59ff17d558c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The next flight from New Delhi (DEL) to Mumbai (BOM) is on March 23, 2024. The flight is operated by KLM with flight number KL643.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response3.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737194cb-646d-40bd-8a8d-771d35532cb0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### What is Funtion Calling actully => Connects large language models to external tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7ecd0d-21c8-4dc0-b987-d1a2890837ce",
   "metadata": {},
   "source": [
    "# Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6bdf207e-05f0-4d08-b1d9-6e2340fd603e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "11f6df1d-2a1a-4101-ba3a-83c321804b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e251a56f-71d6-49f6-bc33-0d757c2d0ba4",
   "metadata": {},
   "source": [
    "llm = OpenAI(openai_api_key = my_key)\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n",
    "\n",
    "llm_chain.run(question)\n",
    "\n",
    "' Justin Bieber was born on March 1, 1994. The Super Bowl is usually played in February, so we need to look at the Super Bowl that was played in 1994. \\n\\nIn 1994, the Super Bowl was played on January 30, 1994. The teams that played in that Super Bowl were the Dallas Cowboys and the Buffalo Bills. \\n\\nThe winner of that Super Bowl was the Dallas Cowboys, who defeated the Buffalo Bills with a score of 30-13. \\n\\nSo, the answer to this question is the Dallas Cowboys. '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "61ff28ca-87d6-4fdb-af8a-f7ce17bd7410",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(openai_api_key=my_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "128779f4-1c9f-451c-bf06-7532d5df2c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero shot prompting\n",
    "prompt=\"can you tell me total number of country in aisa? can you give me top 10 contry name?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9314286e-17bd-4069-b853-5823ae8aebdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `predict` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 48 countries in Asia. The top 10 countries in Asia, based on population, are:\n",
      "\n",
      "1. China\n",
      "2. India\n",
      "3. Indonesia\n",
      "4. Pakistan\n",
      "5. Bangladesh\n",
      "6. Japan\n",
      "7. Philippines\n",
      "8. Vietnam\n",
      "9. Iran\n",
      "10. Thailand\n"
     ]
    }
   ],
   "source": [
    "# legacy soon. Use above markdowned code\n",
    "print(client.predict(prompt).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8836e285-20ae-40fb-9a54-d2af3b58ac43",
   "metadata": {},
   "source": [
    "# Here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1b26f95e-e6f2-4898-ab91-a7f7265024b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero shot prompting\n",
    "prompt2=\"can you tell me the capital of India?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c4f215e4-d932-4a7f-b03e-a5c257079734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of India is New Delhi.'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.predict(prompt2).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "55410e92-2b28-4bea-9c12-8f903a870fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt3=\"what exactly are tokens , vector ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bbeee345-2b23-4735-8e14-4bb036e5382e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tokens are small units of language that have meaning and can be used to build sentences and phrases. In computer programming, tokens refer to the individual words, symbols, or other elements that make up a programming language. They are the basic building blocks of code.\\n\\nVectors, on the other hand, refer to a type of data structure in computer programming. They are arrays that can hold multiple values of the same data type. Vectors are commonly used to store and manipulate large amounts of data efficiently, such as in data analysis and machine learning applications.'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.predict(prompt3).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9263093e-2b9d-470b-a097-c794f45793d2",
   "metadata": {},
   "source": [
    "### Prompt Templates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dfd52ca3-d2cf-45be-b36b-211407e09b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ceade1b0-e028-4dc0-b834-de6446a1d952",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables=[\"country\"],  # for input value\n",
    "    template=\"can you tell me the capital of {country}?\" # actual question\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a1a5d8fe-f199-4e54-bda0-6bcb73718446",
   "metadata": {},
   "outputs": [],
   "source": [
    "propmt1=prompt_template_name.format(country=\"India\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ae894343-cab8-4264-8d65-dd20b3e2135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "propmt2=prompt_template_name.format(country=\"hina\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "90647aca-5f11-4f6f-b048-d14ee18921b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of India is New Delhi.'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.predict(propmt1).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0d86979b-0b71-416f-80cc-df58f176aa97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of China is Beijing.'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.predict(propmt2).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6e241d41-c6f1-4386-8ec3-b4a6bfd068bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way\n",
    "prompt=PromptTemplate.from_template(\"what is a good name for a compnay that makes a {product}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3d55cf84-aaf4-42d6-92f2-186215dd7b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt3=prompt.format(product=\"toys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1ed3fb9e-da50-475c-af41-9da532ce3c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Playful Creations\" or \"Imaginative Toys Co.\"\n"
     ]
    }
   ],
   "source": [
    "print(client.predict(prompt3).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33df71c-9df5-4578-99e6-0c03fd1ee898",
   "metadata": {},
   "source": [
    "## for extracting a real time info I am going to user serp api\n",
    "\n",
    "## now by using this serp api I will call google-search-engine \n",
    "\n",
    "## and I will extract the information in real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "43095215-43ca-40eb-a4a3-ffee91c50104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-search-results\n",
      "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: requests in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from google-search-results) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from requests->google-search-results) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from requests->google-search-results) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from requests->google-search-results) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from requests->google-search-results) (2024.2.2)\n",
      "Building wheels for collected packages: google-search-results\n",
      "  Building wheel for google-search-results (setup.py): started\n",
      "  Building wheel for google-search-results (setup.py): finished with status 'done'\n",
      "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32077 sha256=dfb6da257afa1d138f6901e376f92f0cd313ff0050879901fdf094e2b6b748fe\n",
      "  Stored in directory: c:\\users\\bibas\\appdata\\local\\pip\\cache\\wheels\\29\\75\\71\\9bf68178a74593837f73b6e9d9a070d45d308bddfd2e95290a\n",
      "Successfully built google-search-results\n",
      "Installing collected packages: google-search-results\n",
      "Successfully installed google-search-results-2.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "05524d52-194b-42f0-93e8-441bc23897c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "serpapi_key=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "775a74bd-de13-4019-b0ef-1cd57e12c259",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType\n",
    "from langchain.agents import load_tools \n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bb708a55-6a0b-4432-a1d8-bab48ef1c37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "client=OpenAI(openai_api_key=my_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1e71d63d-0a96-482f-bbcb-298493299b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool=load_tools([\"serpapi\"],serpapi_api_key=serpapi_key,llm=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9147615b-4323-47c1-9626-efdccd63a425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "agent = initialize_agent(tool, client, agent = AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "be6c9e5b-b872-4bab-8dfb-cab008f3b5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should think about searching for the latest news on the cricket worldcup.\n",
      "Action: Search\n",
      "Action Input: \"cricket worldcup recent winner\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m{'title': 'ICC Cricket World Cup', 'thumbnail': 'https://serpapi.com/searches/65fe28d6a74fe06669ca12f1/images/2dbca4c68580a752f5760c5a19753c7405b872da9093743957d197ecfff87e37.png', 'games': [{'tournament': 'ICC Cricket World Cup', 'stadium': 'Narendra Modi Stadium', 'date': 'Nov 19, 23', 'teams': [{'name': 'India', 'score': '240 (50)', 'thumbnail': 'https://serpapi.com/searches/65fe28d6a74fe06669ca12f1/images/2dbca4c68580a752f1c33d12f003467b6b6230a07a2db798a771e1405dc6573f5faf8e53c03f0e7c472fa7bdc57ff280.png'}, {'name': 'Australia', 'score': '241/4 (43)', 'thumbnail': 'https://serpapi.com/searches/65fe28d6a74fe06669ca12f1/images/2dbca4c68580a752f1c33d12f003467b6b6230a07a2db7987ae76aa833000d64b3af5fbcac5c7b3059bbab49cc0f06f3.png'}], 'status': 'AUS won by 6 wickets (42 balls left)'}, {'tournament': 'ICC Cricket World Cup', 'stadium': 'Eden Gardens', 'date': 'Nov 16, 23', 'teams': [{'name': 'South Africa', 'score': '212 (49.4)', 'thumbnail': 'https://serpapi.com/searches/65fe28d6a74fe06669ca12f1/images/2dbca4c68580a752f1c33d12f003467b22aee7fe65ff1c1802f1be9c4516f662af15dacd3613bf4f82980484a85d909d.png'}, {'name': 'Australia', 'score': '215/7 (47.2)', 'thumbnail': 'https://serpapi.com/searches/65fe28d6a74fe06669ca12f1/images/2dbca4c68580a752f1c33d12f003467b22aee7fe65ff1c189a827a0b37a065c466b27b7b5077313136c2de66c04b0f1d.png'}], 'status': 'AUS won by 3 wickets (16 balls left)'}, {'tournament': 'ICC Cricket World Cup', 'stadium': 'Wankhede Stadium', 'date': 'Nov 15, 23', 'teams': [{'name': 'India', 'score': '397/4 (50)', 'thumbnail': 'https://serpapi.com/searches/65fe28d6a74fe06669ca12f1/images/2dbca4c68580a752f1c33d12f003467bddc843921c945d09ac54420a24f4c98e242c3d077acb9954fc030d5a07a8f6e2.png'}, {'name': 'New Zealand', 'score': '327 (48.5)', 'thumbnail': 'https://serpapi.com/searches/65fe28d6a74fe06669ca12f1/images/2dbca4c68580a752f1c33d12f003467bddc843921c945d09f89fd81922f4b7d148ebef7dad3e8952549f9b142d3e7ded.png'}], 'status': 'IND won by 70 runs'}, {'tournament': 'ICC Cricket World Cup', 'stadium': 'M. Chinnaswamy Stadium', 'date': 'Nov 12, 23', 'teams': [{'name': 'India', 'score': '410/4 (50)', 'thumbnail': 'https://serpapi.com/searches/65fe28d6a74fe06669ca12f1/images/2dbca4c68580a752f1c33d12f003467ba439b52b65c6a9df9820a23a155d5fb517ff73e3c479d876ff42db90aed1bf0c.png'}, {'name': 'Netherlands', 'score': '250 (47.5)', 'thumbnail': 'https://serpapi.com/searches/65fe28d6a74fe06669ca12f1/images/2dbca4c68580a752f1c33d12f003467ba439b52b65c6a9df5d671e85ec1e0266d6383c1fd2cd18979797b2040816c83e.png'}], 'status': 'IND won by 160 runs'}, {'tournament': 'ICC Cricket World Cup', 'stadium': 'Eden Gardens', 'date': 'Nov 11, 23', 'teams': [{'name': 'England', 'score': '337/9 (50)', 'thumbnail': 'https://serpapi.com/searches/65fe28d6a74fe06669ca12f1/images/2dbca4c68580a752f1c33d12f003467b7ca5038f5a5d51a9ac43287975000f84218f18fc6a1991226914c58950b207bf.png'}, {'name': 'Pakistan', 'score': '244 (43.3)', 'thumbnail': 'https://serpapi.com/searches/65fe28d6a74fe06669ca12f1/images/2dbca4c68580a752f1c33d12f003467b7ca5038f5a5d51a9231ca5db4de9217f16cba5d935ca0f9b3421729e0a056c41.png'}], 'status': 'ENG won by 93 runs'}, {'tournament': 'ICC Cricket World Cup', 'stadium': 'Maharashtra Cricket Association Stadium', 'date': 'Nov 10, 23', 'teams': [{'name': 'Bangladesh', 'score': '306/8 (50)', 'thumbnail': 'https://serpapi.com/searches/65fe28d6a74fe06669ca12f1/images/2dbca4c68580a752f1c33d12f003467bd697482bf541ba56ed48644da6bd6252fb102eecc4c0b2493a57d5decad7d079.png'}, {'name': 'Australia', 'score': '307/2 (44.4)', 'thumbnail': 'https://serpapi.com/searches/65fe28d6a74fe06669ca12f1/images/2dbca4c68580a752f1c33d12f003467bd697482bf541ba562cc8261a517d4c13938eff278ef0e16292120423f3764810.png'}], 'status': 'AUS won by 8 wickets (32 balls left)'}]}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should check the most recent date of the matches to determine the winner.\n",
      "Action: Get the date of the latest match\n",
      "Action Input: \"ICC Cricket World Cup latest match date\"\u001b[0m\n",
      "Observation: Get the date of the latest match is not a valid tool, try one of [Search].\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should use the search tool to find the date of the latest match.\n",
      "Action: Search\n",
      "Action Input: \"cricket worldcup latest match date\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m{'title': 'ICC Cricket World Cup', 'thumbnail': 'https://serpapi.com/searches/65fe28dec9de45dac3ff6956/images/7281d97a0be0c4237e5ad9a91d7c4053dc751f042dd9a5fe98f94fc50ad129a9.png', 'games': [{'tournament': 'ICC Cricket World Cup', 'stadium': 'Narendra Modi Stadium', 'date': 'Nov 19, 23', 'teams': [{'name': 'India', 'score': '240 (50)', 'thumbnail': 'https://serpapi.com/searches/65fe28dec9de45dac3ff6956/images/7281d97a0be0c423a628c862c82491cc29b18a997141ba714a3dc7388b6da0ba611996838539af759760a42318c502a8.png'}, {'name': 'Australia', 'score': '241/4 (43)', 'thumbnail': 'https://serpapi.com/searches/65fe28dec9de45dac3ff6956/images/7281d97a0be0c423a628c862c82491cc29b18a997141ba71b8d06e335673889bb143a8c4c64cf318d743457455448afd.png'}], 'status': 'AUS won by 6 wickets (42 balls left)'}, {'tournament': 'ICC Cricket World Cup', 'stadium': 'Eden Gardens', 'date': 'Nov 16, 23', 'teams': [{'name': 'South Africa', 'score': '212 (49.4)', 'thumbnail': 'https://serpapi.com/searches/65fe28dec9de45dac3ff6956/images/7281d97a0be0c423a628c862c82491cc50a65629dd7501392c5bfa926868cbba1eba4e2f1b4b103de4fd8cf8ce4c2d15.png'}, {'name': 'Australia', 'score': '215/7 (47.2)', 'thumbnail': 'https://serpapi.com/searches/65fe28dec9de45dac3ff6956/images/7281d97a0be0c423a628c862c82491cc50a65629dd7501396310b07527070490c2006e0aa32db501bccc34e62e10bbe1.png'}], 'status': 'AUS won by 3 wickets (16 balls left)'}, {'tournament': 'ICC Cricket World Cup', 'stadium': 'Wankhede Stadium', 'date': 'Nov 15, 23', 'teams': [{'name': 'India', 'score': '397/4 (50)', 'thumbnail': 'https://serpapi.com/searches/65fe28dec9de45dac3ff6956/images/7281d97a0be0c423a628c862c82491cc458b3e31bfc290a01beb496fb206dcdbe8418c6cf1430ea1848e3894cb4de97a.png'}, {'name': 'New Zealand', 'score': '327 (48.5)', 'thumbnail': 'https://serpapi.com/searches/65fe28dec9de45dac3ff6956/images/7281d97a0be0c423a628c862c82491cc458b3e31bfc290a0220ddeb0097587cc10f62715f2a057406eb5a6476755b56b.png'}], 'status': 'IND won by 70 runs'}, {'tournament': 'ICC Cricket World Cup', 'stadium': 'M. Chinnaswamy Stadium', 'date': 'Nov 12, 23', 'teams': [{'name': 'India', 'score': '410/4 (50)', 'thumbnail': 'https://serpapi.com/searches/65fe28dec9de45dac3ff6956/images/7281d97a0be0c423a628c862c82491cc9e42a53c60776a0e70ced0ceb7807335691dd3c9a50a949d0e78dbeb0d129486.png'}, {'name': 'Netherlands', 'score': '250 (47.5)', 'thumbnail': 'https://serpapi.com/searches/65fe28dec9de45dac3ff6956/images/7281d97a0be0c423a628c862c82491cc9e42a53c60776a0e30baec4b5b7607250bdcd4db80be8405425e210f652961ca.png'}], 'status': 'IND won by 160 runs'}, {'tournament': 'ICC Cricket World Cup', 'stadium': 'Eden Gardens', 'date': 'Nov 11, 23', 'teams': [{'name': 'England', 'score': '337/9 (50)', 'thumbnail': 'https://serpapi.com/searches/65fe28dec9de45dac3ff6956/images/7281d97a0be0c423a628c862c82491cc89b2f040ba5b2e332aafde29824d55acd05106326be4b05174360c5cba246499.png'}, {'name': 'Pakistan', 'score': '244 (43.3)', 'thumbnail': 'https://serpapi.com/searches/65fe28dec9de45dac3ff6956/images/7281d97a0be0c423a628c862c82491cc89b2f040ba5b2e334f30a85bdb0df4dda874af0476b3404e301336c89c5c2e8a.png'}], 'status': 'ENG won by 93 runs'}, {'tournament': 'ICC Cricket World Cup', 'stadium': 'Maharashtra Cricket Association Stadium', 'date': 'Nov 11, 23', 'teams': [{'name': 'Bangladesh', 'score': '306/8 (50)', 'thumbnail': 'https://serpapi.com/searches/65fe28dec9de45dac3ff6956/images/7281d97a0be0c423a628c862c82491ccdf05daa4cea3ca428966b2d03bcf6145a04141dbd8f025bd12d8b77acd73b28b.png'}, {'name': 'Australia', 'score': '307/2 (44.4)', 'thumbnail': 'https://serpapi.com/searches/65fe28dec9de45dac3ff6956/images/7281d97a0be0c423a628c862c82491ccdf05daa4cea3ca42f26ee829ddef72e3a9e91eafe3939f8036f5f6a6694f3daf.png'}], 'status': 'AUS won by 8 wickets (32 balls left)'}]}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should now have all the information I need to determine the winner of the cricket worldcup.\n",
      "Final Answer: Australia won the 2023 ICC Cricket World Cup.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Australia won the 2023 ICC Cricket World Cup.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"can you tell who won the cricket worldcup recently?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "440b5e6e-4cc2-4196-8803-d8c696cc3c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should think about what to do.\n",
      "Action: Search\n",
      "Action Input: \"top current affairs news\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m['Latest News Today: Breaking news on Politics, Business, Sports, Bollywood, Education, Science. Latest News on Politics, India, Current affairs.', 'Latest world news headlines: International breaking news and current affairs from US, Europe, Middle East, Asia, Africa and more. If it happens around the ...', 'Get the latest news about politics, the economy, health care, the environment, education, Congress and state, local and federal government.', 'Hyderabad: The Congress is attempting to regain its lost political ground in Greater Hyderabad limits by... 22 March 2024 12:26 PM. Dhobis Set High Standards of ...', 'View the latest news and breaking news today for U.S., world, weather, entertainment, politics and health at CNN.com.', 'Reuters.com is your online source for the latest US news stories and current events, ensuring our readers up to date with any breaking news developments.', 'current affairs · All(20341) · Articles(20277) · Videos(3) · Photos(61) ...', 'News, off-beat stories and analysis of German and international affairs. Dive deeper with our features from Europe and beyond. Watch our 24/7 TV stream.', 'Stay informed with top world news today. The Associated Press aims to keep you up-to-date with breaking world news stories around the globe.', 'Get daily updates on latest Current Affairs 2024 for upcoming Bank, SSC, Railway, Defence, UPSC, UPPSC, BPSC, MPPSC, RPSC, KPSC, & all other Govt ...']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should think about what to do next.\n",
      "Action: Search\n",
      "Action Input: \"top current affairs news 2021\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m['\"Sunday Morning\" host Jane Pauley looks back on key events of a dramatic year.', 'Get the latest news about politics, the economy ... 2021,. U.S. Life Expectancy Gains Ground · PHOENIX ... Top News · Madonna to Hold Copacabana Beach Concert ...', 'View the latest news and breaking news today for U.S., world, weather, entertainment, politics and health at CNN.com.', 'Read the latest headlines, breaking news, and videos at APNews.com, the definitive source for independent journalism from every corner of the globe.', 'Your trusted source for breaking news, analysis, exclusive interviews, headlines, and videos at ABCNews.com.', 'Foreign Affairs is the leading magazine for in-depth analysis and debate of foreign policy, geopolitics and international affairs.', 'A Current Affairs subscription is one of the best known ways to improve your life in a hurry. ... Continue browsing the latest Current Affairs ... © 2021 Current ...', 'US News has breaking world news on the war in Iraq and Afghanistan, international business news about the European and Asian economic market, and more.', \"8. Iran's Nuclear Program Advances. · 7. Supply Chains Falter. · 6. The Taliban Return to Power. · 5. Ethiopia's Civil War Worsens. · 4. The Global ...\", 'Breaking news and analysis from TIME.com. Politics, world news, photos, video, tech reviews, health, science and entertainment news.']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should think about what to do next.\n",
      "Action: Search\n",
      "Action Input: \"top current affairs news 2021\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m['\"Sunday Morning\" host Jane Pauley looks back on key events of a dramatic year.', 'Get the latest news about politics, the economy ... 2021,. U.S. Life Expectancy Gains Ground · PHOENIX ... Top News · Madonna to Hold Copacabana Beach Concert ...', 'View the latest news and breaking news today for U.S., world, weather, entertainment, politics and health at CNN.com.', 'Read the latest headlines, breaking news, and videos at APNews.com, the definitive source for independent journalism from every corner of the globe.', 'Your trusted source for breaking news, analysis, exclusive interviews, headlines, and videos at ABCNews.com.', 'Foreign Affairs is the leading magazine for in-depth analysis and debate of foreign policy, geopolitics and international affairs.', 'A Current Affairs subscription is one of the best known ways to improve your life in a hurry. ... Continue browsing the latest Current Affairs ... © 2021 Current ...', 'US News has breaking world news on the war in Iraq and Afghanistan, international business news about the European and Asian economic market, and more.', \"8. Iran's Nuclear Program Advances. · 7. Supply Chains Falter. · 6. The Taliban Return to Power. · 5. Ethiopia's Civil War Worsens. · 4. The Global ...\", 'Breaking news and analysis from TIME.com. Politics, world news, photos, video, tech reviews, health, science and entertainment news.']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: The top 10 current affairs news of 2021 include the latest news on politics, the economy, international events, and more. Some of the top sources for this news include CNN, AP News, ABC News, TIME, and Foreign Affairs.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The top 10 current affairs news of 2021 include the latest news on politics, the economy, international events, and more. Some of the top sources for this news include CNN, AP News, ABC News, TIME, and Foreign Affairs.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Can you tell me 5 top current affairs news?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c1ff761f-b8bd-4bc2-bbd4-18d8d0b1c804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from wikipedia) (4.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from wikipedia) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from beautifulsoup4->wikipedia) (2.5)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py): started\n",
      "  Building wheel for wikipedia (setup.py): finished with status 'done'\n",
      "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11704 sha256=cab23f74207c5fb5185b96157e5f575002e987e267095b3d6511fd44ff75fd9d\n",
      "  Stored in directory: c:\\users\\bibas\\appdata\\local\\pip\\cache\\wheels\\07\\93\\05\\72c05349177dca2e0ba31a33ba4f7907606f7ddef303517c6a\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5923e650-aa14-494a-82a6-0a6590123745",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = load_tools([\"wikipedia\"], llm=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "867b4484-cfb5-4f6c-97d3-e93a70955a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=initialize_agent(tool ,client ,agent = AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5f9cb7c6-8c5c-499b-967e-38b16c95da63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I don't know about the cricket worldcup, so I should use wikipedia\n",
      "Action: wikipedia\n",
      "Action Input: Cricket World Cup\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Cricket World Cup\n",
      "Summary: The Cricket World Cup (officially known as ICC Men's Cricket World Cup) is the international championship of One Day International (ODI) cricket. The event is organised by the sport's governing body, the International Cricket Council (ICC), every four years, with preliminary qualification rounds leading up to a finals tournament. The tournament is one of the world's most viewed sporting events and considered as the \"flagship event of the international cricket calendar\" by the ICC. It is widely considered the pinnacle championship of the sport of cricket.\n",
      "The first World Cup was organised in England in June 1975, with the first ODI cricket match having been played only four years earlier. However, a separate Women's Cricket World Cup had been held two years before the first men's tournament, and a tournament involving multiple international teams had been held as early as 1912, when a triangular tournament of Test matches was played between Australia, England and South Africa. The first three World Cups were held in England. From the 1987 tournament onwards, hosting has been shared between countries under an unofficial rotation system, with fourteen ICC members having hosted at least one match in the tournament.\n",
      "The current format involves a qualification phase, which takes place over the preceding three years, to determine which teams qualify for the tournament phase. In the tournament phase, 10 teams, including the automatically qualifying host nation, compete for the title at venues within the host nation over about a month. In the 2027 edition, the format will be changed to accommodate an expanded 14-team final competition.A total of twenty teams have competed in the 13 editions of the tournament, with ten teams competing in the recent 2023 tournament. Australia has won the tournament six times, India and West Indies twice each, while Pakistan, Sri Lanka and England have won it once each. The best performance by a non-full-member team came when Kenya made the semi-finals of the 2003 tournament.\n",
      "Australia are the current champions after winning the 2023 World Cup in India. The subsequent 2027 World Cup will be held jointly in South Africa, Zimbabwe and Namibia.\n",
      "\n",
      "Page: 2023 Cricket World Cup\n",
      "Summary: The 2023 ICC Men's Cricket World Cup (also referred to as simply the 2023 Cricket World Cup) was the 13th edition of the Cricket World Cup, a quadrennial One Day International (ODI) cricket tournament organized by the International Cricket Council (ICC). It was hosted from 5 October to 19 November 2023 across ten venues in India.\n",
      "The tournament was contested by ten national teams, maintaining the same format used in 2019. In the knockout stage, India and Australia beat New Zealand and South Africa respectively to advance to the final, played on 19 November at Narendra Modi Stadium. Australia won by 6 wickets, winning their sixth Cricket World Cup title.\n",
      "Virat Kohli was the player of the tournament and also scored the most runs; Mohammed Shami was the leading wicket-taker. A total of 1,250,307 spectators attended matches, the highest number in any Cricket World Cup to-date. The tournament final set viewership records in India, with 518 million viewers, and a peak of 57 million streaming viewers.\n",
      "\n",
      "Page: 2011 Cricket World Cup\n",
      "Summary: The 2011 ICC Cricket World Cup was the tenth Cricket World Cup. It was played in India, Sri Lanka, and for the first time in Bangladesh. India won the tournament, defeating Sri Lanka by 6 wickets in the final at Wankhede Stadium in Mumbai, thus becoming the first country to win the Cricket World Cup final on home soil. India's Yuvraj Singh was declared as the player of the tournament. This was the first time in World Cup history that two Asian teams had appeared in the final. It was also the first time since the 1992 World Cup that the final match did not feature Australia.\n",
      "Fourteen national cricket teams took part in this tournament, including 10 full members and four\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The 2011 Cricket World Cup was the tenth edition of the tournament and was won by India, defeating Sri Lanka in the final. It was held in India, Sri Lanka, and Bangladesh and featured 14 national teams.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The 2011 Cricket World Cup was the tenth edition of the tournament and was won by India, defeating Sri Lanka in the final. It was held in India, Sri Lanka, and Bangladesh and featured 14 national teams.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Can you tell me about this recent cricket worldcup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2766d247-6b1d-47b6-be48-c76a52f8aeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should use wikipedia to find information on the current GDP of USA\n",
      "Action: wikipedia\n",
      "Action Input: GDP of USA\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Economy of the United States\n",
      "Summary: The United States is a highly developed/advanced mixed economy. It is the world's largest economy by nominal GDP; it is also the second largest by purchasing power parity (PPP), behind China. It has the world's seventh highest per capita GDP (nominal) and the eighth highest per capita GDP (PPP) as of 2022. The U.S. accounted for 25.4% of the global economy in 2022 in nominal terms, and about 15.6% in PPP terms. The U.S. dollar is the currency of record most used in international transactions and is the world's reserve currency, backed by a large U.S. treasuries market, its role as the reference standard for the petrodollar system, and its linked eurodollar. Several countries use it as their official currency and in others it is the de facto currency.The American economy is fueled by high productivity, well developed transportation infrastructure, and extensive natural resources. Americans have the highest average household and employee income among OECD member states. In 2021, they had the highest median household income. The U.S. has one of the world's highest income inequalities among the developed countries. The largest U.S. trading partners are Canada, Mexico, China, Japan, Germany, South Korea, the United Kingdom, Taiwan, India, and Vietnam. The U.S. is the world's largest importer and second largest exporter. It has free trade agreements with several countries, including Canada and Mexico (through the USMCA), Australia, South Korea, Israel, and several others that are in effect or under negotiation.By 1890, the United States had overtaken the British Empire as the world's most productive economy. It is the world's largest producer of petroleum and natural gas. In 2016, it was the world's largest trading country and second largest manufacturer, with American manufacturing making up a fifth of the global total. The U.S. not only has the largest internal market for goods, but also dominates the services trade. Total U.S. trade was $4.2 trillion in 2018. Of the world's 500 largest companies, 121 are headquartered in the U.S. The U.S. has the world's highest number of billionaires, with total wealth of $3.0 trillion. U.S. commercial banks had $22.9 trillion in assets in December 2022. U.S. global assets under management had more than $30 trillion in assets. During the Great Recession of 2008, the U.S. economy suffered a significant decline. The American Reinvestment and Recovery Act was enacted by the United States Congress, and in the ensuing years the U.S. experienced the longest economic expansion on record by July 2019.The New York Stock Exchange and Nasdaq are the world's largest stock exchanges by market capitalization and trade volume. In 2014, the U.S. economy was ranked first in international ranking on venture capital and global research and development funding. Consumer spending comprised 68% of the U.S. economy in 2022, while its labor share of income was 44% in 2021. The U.S. has the world's largest consumer market. The nation's labor market has attracted immigrants from all over the world and its net migration rate is among the highest in the world. The U.S. is one of the top-performing economies in studies such as the Ease of Doing Business Index, the Global Competitiveness Report, and others.\n",
      "\n",
      "Page: List of U.S. states and territories by GDP\n",
      "Summary: This is a list of U.S. states and territories by gross domestic product (GDP). This article presents the 50 U.S. states and the District of Columbia and their nominal GDP at current prices.\n",
      "The data source for the list is the Bureau of Economic Analysis (BEA) in 2022. The BEA defined GDP by state as \"the sum of value added from all industries in the state.\"Nominal GDP does not take into account differences in the cost of living in different countries, and the results can vary greatly from one year to another based on fluctuations in the exchange rates of the country's currency. Such fluctuations may change a country's ra\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: According to Wikipedia, the current GDP of USA is $22.68 trillion (2021).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'According to Wikipedia, the current GDP of USA is $22.68 trillion (2021).'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"What is current GDP of USA?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15ed9af-799c-43d9-8509-503e4185f02e",
   "metadata": {},
   "source": [
    "# Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceb006f-35ec-44d1-a261-3ba22b301acc",
   "metadata": {},
   "source": [
    "Central to LangChain is a vital component known as LangChain Chains, forming the core connection among one or several large language models (LLMs). In certain sophisticated applications, it becomes necessary to chain LLMs together, either with each other or with other elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "70f91638-3a26-4bc2-8d80-d3cddd8cdf87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAI(client=<openai.resources.completions.Completions object at 0x000002AB67305D90>, async_client=<openai.resources.completions.AsyncCompletions object at 0x000002AB673D79D0>, openai_api_key='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', openai_proxy='')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "57d1ff20-c262-4e08-9dde-888f0fddb803",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt=PromptTemplate.from_template(\"What is a good name for a company that makes {product}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "70191b11-e16b-42f3-82c4-f3b6fdd75605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a028106c-661d-4aa3-98df-c8103ce53c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=LLMChain(llm=client,prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d0b59ea8-f7f3-4247-bbf5-601b3ce25853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vineyard Visions'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"Wine\").strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e264a9-5cc1-470a-afe8-67edb9e0b673",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "16278551-a735-4cc2-9e07-e99cb7d865ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables=['cuisine'],\n",
    "    template=\"I want to open a restaurent for {cuisine} food, suggest a fency name for this\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c074422f-a079-40b0-a143-68c091ec7982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['cuisine'], template='I want to open a restaurent for {cuisine} food, suggest a fency name for this')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "af74b505-4982-4b07-ae7d-6e56e2a45ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm = client ,prompt = prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ae42e04e-17ec-4ecd-a8f2-1bf3a9b0dba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Spice Palace\"'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"Indian\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ed89eba4-d8ae-4d60-a16b-5bf50392cbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm = client, prompt = prompt_template,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a9eeef4a-9a3e-4f83-b885-c895ddba3dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mI want to open a restaurent for american food, suggest a fency name for this\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n\"The Stateside Bistro\"'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"american\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658af667-97a2-4922-90c6-079d2aa678b7",
   "metadata": {},
   "source": [
    "### If we want to combine multiple chain and set a seqence for that we use SimpleSequentialChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e8eceda7-268d-4a48-b810-cc6a33a33fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables = [\"startup_name\"],\n",
    "    template = \"I want to start a startup for {startup-name}, suggest me a good name for this.\"   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "811da727-b4c4-4fc3-b3ba-713b816ba8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_chain = LLMChain(llm = client, prompt = prompt_template_name,  verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "aeb05b2f-ab3d-4a51-b1e8-d260e7741bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables = [\"name\"],\n",
    "    template = \"Suggest some strategies for {name}\"    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8ac990d2-45f3-44f4-a0e6-f476e67ac8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies_chain = LLMChain(llm = client, prompt=prompt_template_items,  verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3b76b314-15a4-4102-b372-563e3e1af646",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = SimpleSequentialChain(chains = [name_chain, strategies_chain],  verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "717d244d-97fa-47da-a208-3e94fb09423c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mI want to start a startup for Artifical Intelligence, suggest me a good name for this.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "\n",
      "1. \"MindForge AI\"\n",
      "2. \"NeuroNexus\"\n",
      "3. \"Intelliverse\"\n",
      "4. \"CogniTech\"\n",
      "5. \"Synapse AI\"\n",
      "6. \"InnovAIte\"\n",
      "7. \"GeniusWorks\"\n",
      "8. \"AImpact\"\n",
      "9. \"ThinkBotics\"\n",
      "10. \"Brainwave Solutions\"\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSuggest some strategies for \n",
      "\n",
      "1. \"MindForge AI\"\n",
      "2. \"NeuroNexus\"\n",
      "3. \"Intelliverse\"\n",
      "4. \"CogniTech\"\n",
      "5. \"Synapse AI\"\n",
      "6. \"InnovAIte\"\n",
      "7. \"GeniusWorks\"\n",
      "8. \"AImpact\"\n",
      "9. \"ThinkBotics\"\n",
      "10. \"Brainwave Solutions\"\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "\n",
      "1. \"MindForge AI\":\n",
      "- Develop a clear brand image that emphasizes innovation and cutting-edge technology.\n",
      "- Utilize social media platforms to showcase the capabilities and impact of MindForge AI.\n",
      "- Offer free trials or demos to potential clients to demonstrate the effectiveness of the AI technology.\n",
      "- Partner with universities and research institutions to showcase the use of MindForge AI in various fields.\n",
      "- Attend and speak at industry conferences and events to network and gain exposure.\n",
      "- Offer personalized and tailored solutions for each client's specific needs.\n",
      "- Utilize targeted advertising to reach potential clients in relevant industries.\n",
      "- Develop case studies and success stories to showcase the real-world applications of MindForge AI.\n",
      "- Offer ongoing support and training to clients to ensure successful implementation and use of the technology.\n",
      "- Invest in research and development to continuously improve and evolve the AI technology.\n",
      "\n",
      "2. \"NeuroNexus\":\n",
      "- Develop a visually appealing and user-friendly website to showcase the technology and services offered.\n",
      "- Partner with healthcare and medical institutions to showcase the use of NeuroNexus in patient care.\n",
      "- Offer workshops and training sessions for healthcare professionals to learn about and use NeuroNexus.\n",
      "- Utilize targeted advertising to reach potential clients in the medical and healthcare industries.\n",
      "- Attend medical conferences and events to network and showcase\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "1. \"MindForge AI\":\n",
      "- Develop a clear brand image that emphasizes innovation and cutting-edge technology.\n",
      "- Utilize social media platforms to showcase the capabilities and impact of MindForge AI.\n",
      "- Offer free trials or demos to potential clients to demonstrate the effectiveness of the AI technology.\n",
      "- Partner with universities and research institutions to showcase the use of MindForge AI in various fields.\n",
      "- Attend and speak at industry conferences and events to network and gain exposure.\n",
      "- Offer personalized and tailored solutions for each client's specific needs.\n",
      "- Utilize targeted advertising to reach potential clients in relevant industries.\n",
      "- Develop case studies and success stories to showcase the real-world applications of MindForge AI.\n",
      "- Offer ongoing support and training to clients to ensure successful implementation and use of the technology.\n",
      "- Invest in research and development to continuously improve and evolve the AI technology.\n",
      "\n",
      "2. \"NeuroNexus\":\n",
      "- Develop a visually appealing and user-friendly website to showcase the technology and services offered.\n",
      "- Partner with healthcare and medical institutions to showcase the use of NeuroNexus in patient care.\n",
      "- Offer workshops and training sessions for healthcare professionals to learn about and use NeuroNexus.\n",
      "- Utilize targeted advertising to reach potential clients in the medical and healthcare industries.\n",
      "- Attend medical conferences and events to network and showcase\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\"Artifical Intelligence\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eac818d-9e90-4504-b425-1de8790408aa",
   "metadata": {},
   "source": [
    "\\# Now lets understand the sequential chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8e7e3ea7-9ca8-4d71-8254-56a39d8ea775",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables = [\"cuisine\"],\n",
    "    template=\"I want to open a restaurant for {cuisine}, suggest a fency name for it\"\n",
    ")\n",
    "\n",
    "name_chain = LLMChain(llm = client, prompt = prompt_template_name, output_key=\"restaurant_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "140898f5-6008-4275-b9df-033a14b5bf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_templates_items = PromptTemplate( \n",
    "    input_variables = [\"restaurant_name\"],\n",
    "    template = \"suggest some menu items for {restaurant_name}\"\n",
    "    \n",
    ")\n",
    "\n",
    "food_items_chain = LLMChain(llm = client, prompt = prompt_templates_items, output_key=\"menu_items\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d25bfb46-96da-41fc-be7a-8fcf5d8cf33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "19fe54e4-e76d-4051-b995-b5fc7f0029af",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = SequentialChain(chains = [name_chain, food_items_chain],\n",
    "    input_variables = [\"cuisine\"],\n",
    "    output_variables = [\"restaurant_name\",\"menu_items\"]\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "22dee2a8-4f5a-4a77-8854-b3055f916f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cuisine': 'indian',\n",
       " 'restaurant_name': '\\n\\n\"Maharaja\\'s Palace\"',\n",
       " 'menu_items': '\\n\\n1. Butter Chicken: A popular Indian dish made with tender chicken in a creamy, buttery tomato-based sauce. \\n\\n2. Tandoori Mixed Grill: A selection of marinated chicken, lamb, and shrimp grilled in a traditional clay oven. \\n\\n3. Vegetable Biryani: Fragrant basmati rice cooked with mixed vegetables and aromatic spices. \\n\\n4. Lamb Rogan Josh: A rich and spicy lamb curry made with a blend of Kashmiri spices. \\n\\n5. Palak Paneer: Fresh spinach and homemade cottage cheese cooked in a flavorful creamy sauce. \\n\\n6. Garlic Naan: Oven-baked flatbread topped with garlic and herbs. \\n\\n7. Samosas: Crispy pastry filled with spiced potatoes and peas, served with chutney for dipping. \\n\\n8. Dal Makhani: Slow-cooked black lentils in a creamy tomato sauce. \\n\\n9. Mango Lassi: A refreshing yogurt-based drink with mango puree and a hint of cardamom. \\n\\n10. Gulab Jamun: Deep-fried milk dumplings soaked in a sweet syrup, a classic Indian dessert.'}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain({\"cuisine\":\"indian\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b31106c-0de3-4494-9ad7-02c2c40df973",
   "metadata": {},
   "source": [
    "# Document loders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "554e54b9-d69e-442b-af9c-09499c47517c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Using cached pypdf-4.1.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.7.4.3 in c:\\users\\bibas\\anaconda3\\envs\\topenai\\lib\\site-packages (from pypdf) (4.10.0)\n",
      "Using cached pypdf-4.1.0-py3-none-any.whl (286 kB)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-4.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d70d4e05-7c8d-4c3a-9e4d-57a30b03ddfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "df717d5b-033b-49f7-b4ef-94f491d5169a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(r\"[yoursPath].pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "436d9347-26f3-4f2a-9be1-aa5e3f927e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.pdf.PyPDFLoader at 0x2ab686fc340>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2cb1a80b-f35c-4bdc-afd3-216104df0025",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b14efe52-5c78-4bfe-b999-2a523a182f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='EfﬁcientNet: Rethinking Model Scaling for Convolutional Neural Networks\\nMingxing Tan1Quoc V . Le1\\nAbstract\\nConvolutional Neural Networks (ConvNets) are\\ncommonly developed at a ﬁxed resource budget,\\nand then scaled up for better accuracy if more\\nresources are available. In this paper, we sys-\\ntematically study model scaling and identify that\\ncarefully balancing network depth, width, and res-\\nolution can lead to better performance. Based\\non this observation, we propose a new scaling\\nmethod that uniformly scales all dimensions of\\ndepth/width/resolution using a simple yet highly\\neffective compound coefﬁcient . We demonstrate\\nthe effectiveness of this method on scaling up\\nMobileNets and ResNet.\\nTo go even further, we use neural architec-\\nture search to design a new baseline network\\nand scale it up to obtain a family of models,\\ncalled EfﬁcientNets , which achieve much\\nbetter accuracy and efﬁciency than previous\\nConvNets. In particular, our EfﬁcientNet-B7\\nachieves state-of-the-art 84.3% top-1 accuracy\\non ImageNet, while being 8.4x smaller and\\n6.1x faster on inference than the best existing\\nConvNet. Our EfﬁcientNets also transfer well and\\nachieve state-of-the-art accuracy on CIFAR-100\\n(91.7%), Flowers (98.8%), and 3 other transfer\\nlearning datasets, with an order of magnitude\\nfewer parameters. Source code is at https:\\n//github.com/tensorflow/tpu/tree/\\nmaster/models/official/efficientnet .\\n1. Introduction\\nScaling up ConvNets is widely used to achieve better accu-\\nracy. For example, ResNet (He et al., 2016) can be scaled\\nup from ResNet-18 to ResNet-200 by using more layers;\\nRecently, GPipe (Huang et al., 2018) achieved 84.3% Ima-\\ngeNet top-1 accuracy by scaling up a baseline model four\\n1Google Research, Brain Team, Mountain View, CA. Corre-\\nspondence to: Mingxing Tan <tanmingxing@google.com >.\\nProceedings of the 36thInternational Conference on Machine\\nLearning , Long Beach, California, PMLR 97, 2019.\\n0 20 40 60 80 100 120 140 160 180\\nNumber of Parameters (Millions)747678808284Imagenet Top-1 Accuracy (%)\\nResNet-34ResNet-50ResNet-152\\nDenseNet-201\\nInception-v2Inception-ResNet-v2\\nNASNet-ANASNet-A\\nResNeXt-101\\nXceptionAmoebaNet-AAmoebaNet-C\\nSENet\\nB0B3B4B5B6EfﬁcientNet-B7Top1 Acc. #Params\\nResNet-152 (He et al., 2016) 77.8% 60M\\nEfﬁcientNet-B1 79.1% 7.8M\\nResNeXt-101 (Xie et al., 2017) 80.9% 84M\\nEfﬁcientNet-B3 81.6% 12M\\nSENet (Hu et al., 2018) 82.7% 146M\\nNASNet-A (Zoph et al., 2018) 82.7% 89M\\nEfﬁcientNet-B4 82.9% 19M\\nGPipe (Huang et al., 2018)†84.3% 556M\\nEfﬁcientNet-B7 84.3% 66M\\n†Not plotted\\nFigure 1. Model Size vs. ImageNet Accuracy. All numbers are\\nfor single-crop, single-model. Our EfﬁcientNets signiﬁcantly out-\\nperform other ConvNets. In particular, EfﬁcientNet-B7 achieves\\nnew state-of-the-art 84.3% top-1 accuracy but being 8.4x smaller\\nand 6.1x faster than GPipe. EfﬁcientNet-B1 is 7.6x smaller and\\n5.7x faster than ResNet-152. Details are in Table 2 and 4.\\ntime larger. However, the process of scaling up ConvNets\\nhas never been well understood and there are currently many\\nways to do it. The most common way is to scale up Con-\\nvNets by their depth (He et al., 2016) or width (Zagoruyko &\\nKomodakis, 2016). Another less common, but increasingly\\npopular, method is to scale up models by image resolution\\n(Huang et al., 2018). In previous work, it is common to scale\\nonly one of the three dimensions – depth, width, and image\\nsize. Though it is possible to scale two or three dimensions\\narbitrarily, arbitrary scaling requires tedious manual tuning\\nand still often yields sub-optimal accuracy and efﬁciency.\\nIn this paper, we want to study and rethink the process\\nof scaling up ConvNets. In particular, we investigate the\\ncentral question: is there a principled method to scale up\\nConvNets that can achieve better accuracy and efﬁciency?\\nOur empirical study shows that it is critical to balance all\\ndimensions of network width/depth/resolution, and surpris-\\ningly such balance can be achieved by simply scaling each', metadata={'source': 'C:\\\\Users\\\\bibas\\\\Downloads\\\\EfficientNet- Rethinking Model Scaling for Convolutional Neural Networks.pdf', 'page': 0}),\n",
       " Document(page_content='Our empirical study shows that it is critical to balance all\\ndimensions of network width/depth/resolution, and surpris-\\ningly such balance can be achieved by simply scaling each\\nof them with constant ratio. Based on this observation, we\\npropose a simple yet effective compound scaling method .\\nUnlike conventional practice that arbitrary scales these fac-\\ntors, our method uniformly scales network width, depth,arXiv:1905.11946v5  [cs.LG]  11 Sep 2020', metadata={'source': 'C:\\\\Users\\\\bibas\\\\Downloads\\\\EfficientNet- Rethinking Model Scaling for Convolutional Neural Networks.pdf', 'page': 0}),\n",
       " Document(page_content='EfﬁcientNet: Rethinking Model Scaling for Convolutional Neural Networks\\n(a) baseline (b) width scaling (c) depth scaling (d) resolution scaling (e) compound scaling #channels \\nlayer_i \\nresolution HxW wider \\ndeeper \\nhigher \\nresolution higher \\nresolution deeper wider \\nFigure 2. Model Scaling. (a) is a baseline network example; (b)-(d) are conventional scaling that only increases one dimension of network\\nwidth, depth, or resolution. (e) is our proposed compound scaling method that uniformly scales all three dimensions with a ﬁxed ratio.\\nand resolution with a set of ﬁxed scaling coefﬁcients. For\\nexample, if we want to use 2Ntimes more computational\\nresources, then we can simply increase the network depth by\\nαN, width byβN, and image size by γN, whereα,β,γ are\\nconstant coefﬁcients determined by a small grid search on\\nthe original small model. Figure 2 illustrates the difference\\nbetween our scaling method and conventional methods.\\nIntuitively, the compound scaling method makes sense be-\\ncause if the input image is bigger, then the network needs\\nmore layers to increase the receptive ﬁeld and more channels\\nto capture more ﬁne-grained patterns on the bigger image. In\\nfact, previous theoretical (Raghu et al., 2017; Lu et al., 2018)\\nand empirical results (Zagoruyko & Komodakis, 2016) both\\nshow that there exists certain relationship between network\\nwidth and depth, but to our best knowledge, we are the\\nﬁrst to empirically quantify the relationship among all three\\ndimensions of network width, depth, and resolution.\\nWe demonstrate that our scaling method work well on exist-\\ning MobileNets (Howard et al., 2017; Sandler et al., 2018)\\nand ResNet (He et al., 2016). Notably, the effectiveness of\\nmodel scaling heavily depends on the baseline network; to\\ngo even further, we use neural architecture search (Zoph\\n& Le, 2017; Tan et al., 2019) to develop a new baseline\\nnetwork, and scale it up to obtain a family of models, called\\nEfﬁcientNets . Figure 1 summarizes the ImageNet perfor-\\nmance, where our EfﬁcientNets signiﬁcantly outperform\\nother ConvNets. In particular, our EfﬁcientNet-B7 surpasses\\nthe best existing GPipe accuracy (Huang et al., 2018), but\\nusing 8.4x fewer parameters and running 6.1x faster on in-\\nference. Compared to the widely used ResNet-50 (He et al.,\\n2016), our EfﬁcientNet-B4 improves the top-1 accuracy\\nfrom 76.3% to 83.0% (+6.7%) with similar FLOPS. Besides\\nImageNet, EfﬁcientNets also transfer well and achieve state-of-the-art accuracy on 5 out of 8 widely used datasets, while\\nreducing parameters by up to 21x than existing ConvNets.\\n2. Related Work\\nConvNet Accuracy: Since AlexNet (Krizhevsky et al.,\\n2012) won the 2012 ImageNet competition, ConvNets have\\nbecome increasingly more accurate by going bigger: while\\nthe 2014 ImageNet winner GoogleNet (Szegedy et al., 2015)\\nachieves 74.8% top-1 accuracy with about 6.8M parameters,\\nthe 2017 ImageNet winner SENet (Hu et al., 2018) achieves\\n82.7% top-1 accuracy with 145M parameters. Recently,\\nGPipe (Huang et al., 2018) further pushes the state-of-the-art\\nImageNet top-1 validation accuracy to 84.3% using 557M\\nparameters: it is so big that it can only be trained with a\\nspecialized pipeline parallelism library by partitioning the\\nnetwork and spreading each part to a different accelera-\\ntor. While these models are mainly designed for ImageNet,\\nrecent studies have shown better ImageNet models also per-\\nform better across a variety of transfer learning datasets\\n(Kornblith et al., 2019), and other computer vision tasks\\nsuch as object detection (He et al., 2016; Tan et al., 2019).\\nAlthough higher accuracy is critical for many applications,\\nwe have already hit the hardware memory limit, and thus\\nfurther accuracy gain needs better efﬁciency.\\nConvNet Efﬁciency: Deep ConvNets are often over-\\nparameterized. Model compression (Han et al., 2016; He\\net al., 2018; Yang et al., 2018) is a common way to re-\\nduce model size by trading accuracy for efﬁciency. As mo-', metadata={'source': 'C:\\\\Users\\\\bibas\\\\Downloads\\\\EfficientNet- Rethinking Model Scaling for Convolutional Neural Networks.pdf', 'page': 1}),\n",
       " Document(page_content='parameterized. Model compression (Han et al., 2016; He\\net al., 2018; Yang et al., 2018) is a common way to re-\\nduce model size by trading accuracy for efﬁciency. As mo-\\nbile phones become ubiquitous, it is also common to hand-\\ncraft efﬁcient mobile-size ConvNets, such as SqueezeNets\\n(Iandola et al., 2016; Gholami et al., 2018), MobileNets\\n(Howard et al., 2017; Sandler et al., 2018), and ShufﬂeNets', metadata={'source': 'C:\\\\Users\\\\bibas\\\\Downloads\\\\EfficientNet- Rethinking Model Scaling for Convolutional Neural Networks.pdf', 'page': 1}),\n",
       " Document(page_content='EfﬁcientNet: Rethinking Model Scaling for Convolutional Neural Networks\\n(Zhang et al., 2018; Ma et al., 2018). Recently, neural archi-\\ntecture search becomes increasingly popular in designing\\nefﬁcient mobile-size ConvNets (Tan et al., 2019; Cai et al.,\\n2019), and achieves even better efﬁciency than hand-crafted\\nmobile ConvNets by extensively tuning the network width,\\ndepth, convolution kernel types and sizes. However, it is\\nunclear how to apply these techniques for larger models that\\nhave much larger design space and much more expensive\\ntuning cost. In this paper, we aim to study model efﬁciency\\nfor super large ConvNets that surpass state-of-the-art accu-\\nracy. To achieve this goal, we resort to model scaling.\\nModel Scaling: There are many ways to scale a Con-\\nvNet for different resource constraints: ResNet (He et al.,\\n2016) can be scaled down (e.g., ResNet-18) or up (e.g.,\\nResNet-200) by adjusting network depth (#layers), while\\nWideResNet (Zagoruyko & Komodakis, 2016) and Mo-\\nbileNets (Howard et al., 2017) can be scaled by network\\nwidth (#channels). It is also well-recognized that bigger\\ninput image size will help accuracy with the overhead of\\nmore FLOPS. Although prior studies (Raghu et al., 2017;\\nLin & Jegelka, 2018; Sharir & Shashua, 2018; Lu et al.,\\n2018) have shown that network depth and width are both\\nimportant for ConvNets’ expressive power, it still remains\\nan open question of how to effectively scale a ConvNet to\\nachieve better efﬁciency and accuracy. Our work systemati-\\ncally and empirically studies ConvNet scaling for all three\\ndimensions of network width, depth, and resolutions.\\n3. Compound Model Scaling\\nIn this section, we will formulate the scaling problem, study\\ndifferent approaches, and propose our new scaling method.\\n3.1. Problem Formulation\\nA ConvNet Layer ican be deﬁned as a function: Yi=\\nFi(Xi), whereFiis the operator, Yiis output tensor, Xiis\\ninput tensor, with tensor shape ⟨Hi,Wi,Ci⟩1, whereHiand\\nWiare spatial dimension and Ciis the channel dimension.\\nA ConvNetNcan be represented by a list of composed lay-\\ners:N=Fk⊙...⊙F2⊙F1(X1) =⨀\\nj=1...kFj(X1). In\\npractice, ConvNet layers are often partitioned into multiple\\nstages and all layers in each stage share the same architec-\\nture: for example, ResNet (He et al., 2016) has ﬁve stages,\\nand all layers in each stage has the same convolutional type\\nexcept the ﬁrst layer performs down-sampling. Therefore,\\nwe can deﬁne a ConvNet as:\\nN=⨀\\ni=1...sFLi\\ni(\\nX⟨Hi,Wi,Ci⟩)\\n(1)\\nwhereFLi\\nidenotes layer Fiis repeatedLitimes in stage i,\\n⟨Hi,Wi,Ci⟩denotes the shape of input tensor Xof layer\\n1For the sake of simplicity, we omit batch dimension.i. Figure 2(a) illustrate a representative ConvNet, where\\nthe spatial dimension is gradually shrunk but the channel\\ndimension is expanded over layers, for example, from initial\\ninput shape⟨224,224,3⟩to ﬁnal output shape ⟨7,7,512⟩.\\nUnlike regular ConvNet designs that mostly focus on ﬁnd-\\ning the best layer architecture Fi, model scaling tries to ex-\\npand the network length ( Li), width (Ci), and/or resolution\\n(Hi,Wi) without changing Fipredeﬁned in the baseline\\nnetwork. By ﬁxing Fi, model scaling simpliﬁes the design\\nproblem for new resource constraints, but it still remains\\na large design space to explore different Li,Ci,Hi,Wifor\\neach layer. In order to further reduce the design space, we\\nrestrict that all layers must be scaled uniformly with con-\\nstant ratio. Our target is to maximize the model accuracy\\nfor any given resource constraints, which can be formulated\\nas an optimization problem:\\nmax\\nd,w,rAccuracy(\\nN(d,w,r ))\\ns.t.N(d,w,r ) =⨀\\ni=1...sˆFd·ˆLi\\ni(\\nX⟨r·ˆHi,r·ˆWi,w·ˆCi⟩)\\nMemory (N)≤target memory\\nFLOPS (N)≤target ﬂops\\n(2)\\nwherew,d,r are coefﬁcients for scaling network width,\\ndepth, and resolution; ˆFi,ˆLi,ˆHi,ˆWi,ˆCiare predeﬁned pa-\\nrameters in baseline network (see Table 1 as an example).\\n3.2. Scaling Dimensions\\nThe main difﬁculty of problem 2 is that the optimal d,w,r\\ndepend on each other and the values change under different', metadata={'source': 'C:\\\\Users\\\\bibas\\\\Downloads\\\\EfficientNet- Rethinking Model Scaling for Convolutional Neural Networks.pdf', 'page': 2}),\n",
       " Document(page_content='rameters in baseline network (see Table 1 as an example).\\n3.2. Scaling Dimensions\\nThe main difﬁculty of problem 2 is that the optimal d,w,r\\ndepend on each other and the values change under different\\nresource constraints. Due to this difﬁculty, conventional\\nmethods mostly scale ConvNets in one of these dimensions:\\nDepth (ddd):Scaling network depth is the most common way\\nused by many ConvNets (He et al., 2016; Huang et al., 2017;\\nSzegedy et al., 2015; 2016). The intuition is that deeper\\nConvNet can capture richer and more complex features, and\\ngeneralize well on new tasks. However, deeper networks\\nare also more difﬁcult to train due to the vanishing gradient\\nproblem (Zagoruyko & Komodakis, 2016). Although sev-\\neral techniques, such as skip connections (He et al., 2016)\\nand batch normalization (Ioffe & Szegedy, 2015), alleviate\\nthe training problem, the accuracy gain of very deep network\\ndiminishes: for example, ResNet-1000 has similar accuracy\\nas ResNet-101 even though it has much more layers. Figure\\n3 (middle) shows our empirical study on scaling a baseline\\nmodel with different depth coefﬁcient d, further suggesting\\nthe diminishing accuracy return for very deep ConvNets.\\nWidth (www):Scaling network width is commonly used for\\nsmall size models (Howard et al., 2017; Sandler et al., 2018;', metadata={'source': 'C:\\\\Users\\\\bibas\\\\Downloads\\\\EfficientNet- Rethinking Model Scaling for Convolutional Neural Networks.pdf', 'page': 2}),\n",
       " Document(page_content='EfﬁcientNet: Rethinking Model Scaling for Convolutional Neural Networks\\n0 2 4 6 8\\nFLOPS (Billions)75767778798081ImageNet Top-1 Accuracy(%)\\nw=1.0w=1.4w=1.8w=2.6w=3.8w=5.0\\n0 1 2 3 4\\nFLOPS (Billions)75767778798081\\nd=1.0d=2.0d=3.0d=4.0d=6.0d=8.0\\n0 1 2 3\\nFLOPS (Billions)75767778798081\\nr=1.0r=1.3r=1.5r=1.7r=1.9r=2.2r=2.5\\nFigure 3. Scaling Up a Baseline Model with Different Network Width ( w), Depth (d), and Resolution ( r) Coefﬁcients. Bigger\\nnetworks with larger width, depth, or resolution tend to achieve higher accuracy, but the accuracy gain quickly saturate after reaching\\n80%, demonstrating the limitation of single dimension scaling. Baseline network is described in Table 1.\\nTan et al., 2019)2. As discussed in (Zagoruyko & Ko-\\nmodakis, 2016), wider networks tend to be able to capture\\nmore ﬁne-grained features and are easier to train. However,\\nextremely wide but shallow networks tend to have difﬁcul-\\nties in capturing higher level features. Our empirical results\\nin Figure 3 (left) show that the accuracy quickly saturates\\nwhen networks become much wider with larger w.\\nResolution ( rrr):With higher resolution input images, Con-\\nvNets can potentially capture more ﬁne-grained patterns.\\nStarting from 224x224 in early ConvNets, modern Con-\\nvNets tend to use 299x299 (Szegedy et al., 2016) or 331x331\\n(Zoph et al., 2018) for better accuracy. Recently, GPipe\\n(Huang et al., 2018) achieves state-of-the-art ImageNet ac-\\ncuracy with 480x480 resolution. Higher resolutions, such as\\n600x600, are also widely used in object detection ConvNets\\n(He et al., 2017; Lin et al., 2017). Figure 3 (right) shows the\\nresults of scaling network resolutions, where indeed higher\\nresolutions improve accuracy, but the accuracy gain dimin-\\nishes for very high resolutions ( r= 1.0denotes resolution\\n224x224 and r= 2.5denotes resolution 560x560).\\nThe above analyses lead us to the ﬁrst observation:\\nObservation 1 – Scaling up any dimension of network\\nwidth, depth, or resolution improves accuracy, but the accu-\\nracy gain diminishes for bigger models.\\n3.3. Compound Scaling\\nWe empirically observe that different scaling dimensions are\\nnot independent. Intuitively, for higher resolution images,\\nwe should increase network depth, such that the larger re-\\nceptive ﬁelds can help capture similar features that include\\nmore pixels in bigger images. Correspondingly, we should\\nalso increase network width when resolution is higher, in\\n2In some literature, scaling number of channels is called “depth\\nmultiplier”, which means the same as our width coefﬁcient w.\\n0 5 10 15 20 25\\nFLOPS (billions)76777879808182ImageNet Top1 Accuracy (%)\\nd=1.0,r=1.0\\nd=1.0,r=1.3\\nd=2.0,r=1.0\\nd=2.0,r=1.3Figure 4. Scaling Network Width for Different Baseline Net-\\nworks . Each dot in a line denotes a model with different width\\ncoefﬁcient (w). All baseline networks are from Table 1. The ﬁrst\\nbaseline network ( d=1.0,r=1.0) has 18 convolutional layers with\\nresolution 224x224, while the last baseline ( d=2.0,r=1.3) has 36\\nlayers with resolution 299x299.\\norder to capture more ﬁne-grained patterns with more pixels\\nin high resolution images. These intuitions suggest that we\\nneed to coordinate and balance different scaling dimensions\\nrather than conventional single-dimension scaling.\\nTo validate our intuitions, we compare width scaling under\\ndifferent network depths and resolutions, as shown in Figure\\n4. If we only scale network width wwithout changing\\ndepth (d=1.0) and resolution ( r=1.0), the accuracy saturates\\nquickly. With deeper ( d=2.0) and higher resolution ( r=2.0),\\nwidth scaling achieves much better accuracy under the same\\nFLOPS cost. These results lead us to the second observation:\\nObservation 2 – In order to pursue better accuracy and\\nefﬁciency, it is critical to balance all dimensions of network\\nwidth, depth, and resolution during ConvNet scaling.', metadata={'source': 'C:\\\\Users\\\\bibas\\\\Downloads\\\\EfficientNet- Rethinking Model Scaling for Convolutional Neural Networks.pdf', 'page': 3}),\n",
       " Document(page_content='EfﬁcientNet: Rethinking Model Scaling for Convolutional Neural Networks\\nIn fact, a few prior work (Zoph et al., 2018; Real et al., 2019)\\nhave already tried to arbitrarily balance network width and\\ndepth, but they all require tedious manual tuning.\\nIn this paper, we propose a new compound scaling method ,\\nwhich use a compound coefﬁcient φto uniformly scales\\nnetwork width, depth, and resolution in a principled way:\\ndepth:d=αφ\\nwidth:w=βφ\\nresolution:r=γφ\\ns.t.α·β2·γ2≈2\\nα≥1,β≥1,γ≥1(3)\\nwhereα,β,γ are constants that can be determined by a\\nsmall grid search. Intuitively, φis a user-speciﬁed coefﬁ-\\ncient that controls how many more resources are available\\nfor model scaling, while α,β,γ specify how to assign these\\nextra resources to network width, depth, and resolution re-\\nspectively. Notably, the FLOPS of a regular convolution op\\nis proportional to d,w2,r2, i.e., doubling network depth\\nwill double FLOPS, but doubling network width or resolu-\\ntion will increase FLOPS by four times. Since convolution\\nops usually dominate the computation cost in ConvNets,\\nscaling a ConvNet with equation 3 will approximately in-\\ncrease total FLOPS by(\\nα·β2·γ2)φ. In this paper, we\\nconstraintα·β2·γ2≈2such that for any new φ, the total\\nFLOPS will approximately3increase by 2φ.\\n4. EfﬁcientNet Architecture\\nSince model scaling does not change layer operators ˆFi\\nin baseline network, having a good baseline network is\\nalso critical. We will evaluate our scaling method using\\nexisting ConvNets, but in order to better demonstrate the\\neffectiveness of our scaling method, we have also developed\\na new mobile-size baseline, called EfﬁcientNet.\\nInspired by (Tan et al., 2019), we develop our baseline net-\\nwork by leveraging a multi-objective neural architecture\\nsearch that optimizes both accuracy and FLOPS. Speciﬁ-\\ncally, we use the same search space as (Tan et al., 2019),\\nand useACC (m)×[FLOPS (m)/T]was the optimization\\ngoal, where ACC (m)andFLOPS (m)denote the accu-\\nracy and FLOPS of model m,Tis the target FLOPS and\\nw=-0.07 is a hyperparameter for controlling the trade-off\\nbetween accuracy and FLOPS. Unlike (Tan et al., 2019;\\nCai et al., 2019), here we optimize FLOPS rather than la-\\ntency since we are not targeting any speciﬁc hardware de-\\nvice. Our search produces an efﬁcient network, which we\\nname EfﬁcientNet-B0. Since we use the same search space\\nas (Tan et al., 2019), the architecture is similar to Mnas-\\n3FLOPS may differ from theoretical value due to rounding.Table 1. EfﬁcientNet-B0 baseline network – Each row describes\\na stageiwith ˆLilayers, with input resolution ⟨ˆHi,ˆWi⟩and output\\nchannels ˆCi. Notations are adopted from equation 2.\\nStage Operator Resolution #Channels #Layers\\ni ˆFiˆHi×ˆWiˆCiˆLi\\n1 Conv3x3 224×224 32 1\\n2 MBConv1, k3x3 112×112 16 1\\n3 MBConv6, k3x3 112×112 24 2\\n4 MBConv6, k5x5 56×56 40 2\\n5 MBConv6, k3x3 28×28 80 3\\n6 MBConv6, k5x5 14×14 112 3\\n7 MBConv6, k5x5 14×14 192 4\\n8 MBConv6, k3x3 7×7 320 1\\n9 Conv1x1 & Pooling & FC 7×7 1280 1\\nNet, except our EfﬁcientNet-B0 is slightly bigger due to\\nthe larger FLOPS target (our FLOPS target is 400M). Ta-\\nble 1 shows the architecture of EfﬁcientNet-B0. Its main\\nbuilding block is mobile inverted bottleneck MBConv (San-\\ndler et al., 2018; Tan et al., 2019), to which we also add\\nsqueeze-and-excitation optimization (Hu et al., 2018).\\nStarting from the baseline EfﬁcientNet-B0, we apply our\\ncompound scaling method to scale it up with two steps:\\n•STEP 1: we ﬁrst ﬁx φ= 1, assuming twice more re-\\nsources available, and do a small grid search of α,β,γ\\nbased on Equation 2 and 3. In particular, we ﬁnd\\nthe best values for EfﬁcientNet-B0 are α= 1.2,β=\\n1.1,γ= 1.15, under constraint of α·β2·γ2≈2.\\n•STEP 2: we then ﬁx α,β,γ as constants and scale up\\nbaseline network with different φusing Equation 3, to\\nobtain EfﬁcientNet-B1 to B7 (Details in Table 2).\\nNotably, it is possible to achieve even better performance by\\nsearching for α,β,γ directly around a large model, but the\\nsearch cost becomes prohibitively more expensive on larger', metadata={'source': 'C:\\\\Users\\\\bibas\\\\Downloads\\\\EfficientNet- Rethinking Model Scaling for Convolutional Neural Networks.pdf', 'page': 4}),\n",
       " Document(page_content='Notably, it is possible to achieve even better performance by\\nsearching for α,β,γ directly around a large model, but the\\nsearch cost becomes prohibitively more expensive on larger\\nmodels. Our method solves this issue by only doing search\\nonce on the small baseline network (step 1), and then use\\nthe same scaling coefﬁcients for all other models (step 2).\\n5. Experiments\\nIn this section, we will ﬁrst evaluate our scaling method on\\nexisting ConvNets and the new proposed EfﬁcientNets.\\n5.1. Scaling Up MobileNets and ResNets\\nAs a proof of concept, we ﬁrst apply our scaling method\\nto the widely-used MobileNets (Howard et al., 2017; San-\\ndler et al., 2018) and ResNet (He et al., 2016). Table 3\\nshows the ImageNet results of scaling them in different\\nways. Compared to other single-dimension scaling methods,\\nour compound scaling method improves the accuracy on all\\nthese models, suggesting the effectiveness of our proposed\\nscaling method for general existing ConvNets.', metadata={'source': 'C:\\\\Users\\\\bibas\\\\Downloads\\\\EfficientNet- Rethinking Model Scaling for Convolutional Neural Networks.pdf', 'page': 4}),\n",
       " Document(page_content='EfﬁcientNet: Rethinking Model Scaling for Convolutional Neural Networks\\nTable 2. EfﬁcientNet Performance Results on ImageNet (Russakovsky et al., 2015). All EfﬁcientNet models are scaled from our\\nbaseline EfﬁcientNet-B0 using different compound coefﬁcient φin Equation 3. ConvNets with similar top-1/top-5 accuracy are grouped\\ntogether for efﬁciency comparison. Our scaled EfﬁcientNet models consistently reduce parameters and FLOPS by an order of magnitude\\n(up to 8.4x parameter reduction and up to 16x FLOPS reduction) than existing ConvNets.\\nModel Top-1 Acc. Top-5 Acc. #Params Ratio-to-EfﬁcientNet #FLOPs Ratio-to-EfﬁcientNet\\nEfﬁcientNet-B0 77.1% 93.3% 5.3M 1x 0.39B 1x\\nResNet-50 (He et al., 2016) 76.0% 93.0% 26M 4.9x 4.1B 11x\\nDenseNet-169 (Huang et al., 2017) 76.2% 93.2% 14M 2.6x 3.5B 8.9x\\nEfﬁcientNet-B1 79.1% 94.4% 7.8M 1x 0.70B 1x\\nResNet-152 (He et al., 2016) 77.8% 93.8% 60M 7.6x 11B 16x\\nDenseNet-264 (Huang et al., 2017) 77.9% 93.9% 34M 4.3x 6.0B 8.6x\\nInception-v3 (Szegedy et al., 2016) 78.8% 94.4% 24M 3.0x 5.7B 8.1x\\nXception (Chollet, 2017) 79.0% 94.5% 23M 3.0x 8.4B 12x\\nEfﬁcientNet-B2 80.1% 94.9% 9.2M 1x 1.0B 1x\\nInception-v4 (Szegedy et al., 2017) 80.0% 95.0% 48M 5.2x 13B 13x\\nInception-resnet-v2 (Szegedy et al., 2017) 80.1% 95.1% 56M 6.1x 13B 13x\\nEfﬁcientNet-B3 81.6% 95.7% 12M 1x 1.8B 1x\\nResNeXt-101 (Xie et al., 2017) 80.9% 95.6% 84M 7.0x 32B 18x\\nPolyNet (Zhang et al., 2017) 81.3% 95.8% 92M 7.7x 35B 19x\\nEfﬁcientNet-B4 82.9% 96.4% 19M 1x 4.2B 1x\\nSENet (Hu et al., 2018) 82.7% 96.2% 146M 7.7x 42B 10x\\nNASNet-A (Zoph et al., 2018) 82.7% 96.2% 89M 4.7x 24B 5.7x\\nAmoebaNet-A (Real et al., 2019) 82.8% 96.1% 87M 4.6x 23B 5.5x\\nPNASNet (Liu et al., 2018) 82.9% 96.2% 86M 4.5x 23B 6.0x\\nEfﬁcientNet-B5 83.6% 96.7% 30M 1x 9.9B 1x\\nAmoebaNet-C (Cubuk et al., 2019) 83.5% 96.5% 155M 5.2x 41B 4.1x\\nEfﬁcientNet-B6 84.0% 96.8% 43M 1x 19B 1x\\nEfﬁcientNet-B7 84.3% 97.0% 66M 1x 37B 1x\\nGPipe (Huang et al., 2018) 84.3% 97.0% 557M 8.4x - -\\nWe omit ensemble and multi-crop models (Hu et al., 2018), or models pretrained on 3.5B Instagram images (Mahajan et al., 2018).\\nTable 3. Scaling Up MobileNets and ResNet.\\nModel FLOPS Top-1 Acc.\\nBaseline MobileNetV1 (Howard et al., 2017) 0.6B 70.6%\\nScale MobileNetV1 by width ( w=2) 2.2B 74.2%\\nScale MobileNetV1 by resolution ( r=2) 2.2B 72.7%\\ncompound scale ( ddd=1.4,www=1.2,rrr=1.3) 2.3B 75.6%\\nBaseline MobileNetV2 (Sandler et al., 2018) 0.3B 72.0%\\nScale MobileNetV2 by depth ( d=4) 1.2B 76.8%\\nScale MobileNetV2 by width ( w=2) 1.1B 76.4%\\nScale MobileNetV2 by resolution ( r=2) 1.2B 74.8%\\nMobileNetV2 compound scale 1.3B 77.4%\\nBaseline ResNet-50 (He et al., 2016) 4.1B 76.0%\\nScale ResNet-50 by depth ( d=4) 16.2B 78.1%\\nScale ResNet-50 by width ( w=2) 14.7B 77.7%\\nScale ResNet-50 by resolution ( r=2) 16.4B 77.5%\\nResNet-50 compound scale 16.7B 78.8%\\nTable 4. Inference Latency Comparison – Latency is measured\\nwith batch size 1 on a single core of Intel Xeon CPU E5-2690.\\nAcc. @ Latency Acc. @ Latency\\nResNet-152 77.8% @ 0.554s GPipe 84.3% @ 19.0s\\nEfﬁcientNet-B1 78.8% @ 0.098s EfﬁcientNet-B7 84.4% @ 3.1s\\nSpeedup 5.7x Speedup 6.1x\\n0 5 10 15 20 25 30 35 40 45\\nFLOPS (Billions)747678808284Imagenet Top-1 Accuracy (%)\\nResNet-34ResNet-50ResNet-152\\nDenseNet-201\\nInception-v2Inception-ResNet-v2\\nNASNet-ANASNet-A\\nResNeXt-101\\nXceptionAmeobaNet-AAmoebaNet-C\\nSENet\\nB0B3B4B5EfﬁcientNet-B6Top1 Acc. FLOPS\\nResNet-152 (Xie et al., 2017) 77.8% 11B\\nEfﬁcientNet-B1 79.1% 0.7B\\nResNeXt-101 (Xie et al., 2017) 80.9% 32B\\nEfﬁcientNet-B3 81.6% 1.8B\\nSENet (Hu et al., 2018) 82.7% 42B\\nNASNet-A (Zoph et al., 2018) 80.7% 24B\\nEfﬁcientNet-B4 82.9% 4.2B\\nAmeobaNet-C (Cubuk et al., 2019) 83.5% 41B\\nEfﬁcientNet-B5 83.6% 9.9B\\nFigure 5. FLOPS vs. ImageNet Accuracy – Similar to Figure 1\\nexcept it compares FLOPS rather than model size.\\n5.2. ImageNet Results for EfﬁcientNet\\nWe train our EfﬁcientNet models on ImageNet using simi-\\nlar settings as (Tan et al., 2019): RMSProp optimizer with\\ndecay 0.9 and momentum 0.9; batch norm momentum 0.99;', metadata={'source': 'C:\\\\Users\\\\bibas\\\\Downloads\\\\EfficientNet- Rethinking Model Scaling for Convolutional Neural Networks.pdf', 'page': 5}),\n",
       " Document(page_content='EfﬁcientNet: Rethinking Model Scaling for Convolutional Neural Networks\\nTable 5. EfﬁcientNet Performance Results on Transfer Learning Datasets . Our scaled EfﬁcientNet models achieve new state-of-the-\\nart accuracy for 5 out of 8 datasets, with 9.6x fewer parameters on average.\\nComparison to best public-available results Comparison to best reported results\\nModel Acc. #Param Our Model Acc. #Param(ratio) Model Acc. #Param Our Model Acc. #Param(ratio)\\nCIFAR-10 NASNet-A 98.0% 85M EfﬁcientNet-B0 98.1% 4M (21x)†Gpipe 99.0% 556M EfﬁcientNet-B7 98.9% 64M (8.7x)\\nCIFAR-100 NASNet-A 87.5% 85M EfﬁcientNet-B0 88.1% 4M (21x) Gpipe 91.3% 556M EfﬁcientNet-B7 91.7% 64M (8.7x)\\nBirdsnap Inception-v4 81.8% 41M EfﬁcientNet-B5 82.0% 28M (1.5x) GPipe 83.6% 556M EfﬁcientNet-B7 84.3% 64M (8.7x)\\nStanford Cars Inception-v4 93.4% 41M EfﬁcientNet-B3 93.6% 10M (4.1x)‡DAT 94.8% - EfﬁcientNet-B7 94.7% -\\nFlowers Inception-v4 98.5% 41M EfﬁcientNet-B5 98.5% 28M (1.5x) DAT 97.7% - EfﬁcientNet-B7 98.8% -\\nFGVC Aircraft Inception-v4 90.9% 41M EfﬁcientNet-B3 90.7% 10M (4.1x) DAT 92.9% - EfﬁcientNet-B7 92.9% -\\nOxford-IIIT Pets ResNet-152 94.5% 58M EfﬁcientNet-B4 94.8% 17M (5.6x) GPipe 95.9% 556M EfﬁcientNet-B6 95.4% 41M (14x)\\nFood-101 Inception-v4 90.8% 41M EfﬁcientNet-B4 91.5% 17M (2.4x) GPipe 93.0% 556M EfﬁcientNet-B7 93.0% 64M (8.7x)\\nGeo-Mean (4.7x) (9.6x)\\n†GPipe (Huang et al., 2018) trains giant models with specialized pipeline parallelism library.\\n‡DAT denotes domain adaptive transfer learning (Ngiam et al., 2018). Here we only compare ImageNet-based transfer learning results.\\nTransfer accuracy and #params for NASNet (Zoph et al., 2018), Inception-v4 (Szegedy et al., 2017), ResNet-152 (He et al., 2016) are from (Kornblith et al., 2019).\\n0.0 0.2 0.4 0.6 0.8 1.0\\nNumber of Parameters (Millions, log-scale)0.00.20.40.60.81.0\\n10110210396979899Accuracy(%)CIFAR10\\n1011021038486889092CIFAR100\\n10110210370758085Birdsnap\\n10110210391929394Stanford Cars\\n10110210397.097.598.098.5Accuracy(%)Flowers\\n10110210382.585.087.590.092.5FGVC Aircraft\\n101102103929496Oxford-IIIT Pets\\n10110210386889092Food-101\\nDenseNet-201\\nGPIPE\\nInception-ResNet-v2ResNet-50\\nResNet-101\\nDenseNet-169Inception-v1\\nInception-v3\\nInception-v4ResNet-152\\nDenseNet-121NASNet-A\\nEfﬁcientNet\\nFigure 6. Model Parameters vs. Transfer Learning Accuracy – All models are pretrained on ImageNet and ﬁnetuned on new datasets.\\nweight decay 1e-5; initial learning rate 0.256 that decays\\nby 0.97 every 2.4 epochs. We also use SiLU (Swish-1) ac-\\ntivation (Ramachandran et al., 2018; Elfwing et al., 2018;\\nHendrycks & Gimpel, 2016), AutoAugment (Cubuk et al.,\\n2019), and stochastic depth (Huang et al., 2016) with sur-\\nvival probability 0.8. As commonly known that bigger mod-\\nels need more regularization, we linearly increase dropout\\n(Srivastava et al., 2014) ratio from 0.2 for EfﬁcientNet-B0 to\\n0.5 for B7. We reserve 25K randomly picked images from\\nthetraining set as a minival set, and perform early\\nstopping on this minival ; we then evaluate the early-\\nstopped checkpoint on the original validation set to\\nreport the ﬁnal validation accuracy.\\nTable 2 shows the performance of all EfﬁcientNet models\\nthat are scaled from the same baseline EfﬁcientNet-B0. Our\\nEfﬁcientNet models generally use an order of magnitude\\nfewer parameters and FLOPS than other ConvNets with\\nsimilar accuracy. In particular, our EfﬁcientNet-B7 achieves\\n84.3% top1 accuracy with 66M parameters and 37B FLOPS,being more accurate but 8.4x smaller than the previous\\nbest GPipe (Huang et al., 2018). These gains come from\\nboth better architectures, better scaling, and better training\\nsettings that are customized for EfﬁcientNet.\\nFigure 1 and Figure 5 illustrates the parameters-accuracy\\nand FLOPS-accuracy curve for representative ConvNets,\\nwhere our scaled EfﬁcientNet models achieve better accu-\\nracy with much fewer parameters and FLOPS than other\\nConvNets. Notably, our EfﬁcientNet models are not only\\nsmall, but also computational cheaper. For example, our', metadata={'source': 'C:\\\\Users\\\\bibas\\\\Downloads\\\\EfficientNet- Rethinking Model Scaling for Convolutional Neural Networks.pdf', 'page': 6}),\n",
       " Document(page_content='racy with much fewer parameters and FLOPS than other\\nConvNets. Notably, our EfﬁcientNet models are not only\\nsmall, but also computational cheaper. For example, our\\nEfﬁcientNet-B3 achieves higher accuracy than ResNeXt-\\n101 (Xie et al., 2017) using 18x fewer FLOPS .\\nTo validate the latency, we have also measured the inference\\nlatency for a few representative CovNets on a real CPU as\\nshown in Table 4, where we report average latency over\\n20 runs. Our EfﬁcientNet-B1 runs 5.7x faster than the\\nwidely used ResNet-152, while EfﬁcientNet-B7 runs about\\n6.1x faster than GPipe (Huang et al., 2018), suggesting our\\nEfﬁcientNets are indeed fast on real hardware.', metadata={'source': 'C:\\\\Users\\\\bibas\\\\Downloads\\\\EfficientNet- Rethinking Model Scaling for Convolutional Neural Networks.pdf', 'page': 6}),\n",
       " Document(page_content='EfﬁcientNet: Rethinking Model Scaling for Convolutional Neural Networks\\nbakeshoporiginal image\\n baseline model\\n deeper (d=4)\\n wider (w=2)\\n higher resolution ( r=2)\\n compound scaling\\nmaze\\nFigure 7. Class Activation Map (CAM) (Zhou et al., 2016) for Models with different scaling methods- Our compound scaling method\\nallows the scaled model (last column) to focus on more relevant regions with more object details. Model details are in Table 7.\\nTable 6. Transfer Learning Datasets .\\nDataset Train Size Test Size #Classes\\nCIFAR-10 (Krizhevsky & Hinton, 2009) 50,000 10,000 10\\nCIFAR-100 (Krizhevsky & Hinton, 2009) 50,000 10,000 100\\nBirdsnap (Berg et al., 2014) 47,386 2,443 500\\nStanford Cars (Krause et al., 2013) 8,144 8,041 196\\nFlowers (Nilsback & Zisserman, 2008) 2,040 6,149 102\\nFGVC Aircraft (Maji et al., 2013) 6,667 3,333 100\\nOxford-IIIT Pets (Parkhi et al., 2012) 3,680 3,369 37\\nFood-101 (Bossard et al., 2014) 75,750 25,250 101\\n5.3. Transfer Learning Results for EfﬁcientNet\\nWe have also evaluated our EfﬁcientNet on a list of com-\\nmonly used transfer learning datasets, as shown in Table\\n6. We borrow the same training settings from (Kornblith\\net al., 2019) and (Huang et al., 2018), which take ImageNet\\npretrained checkpoints and ﬁnetune on new datasets.\\nTable 5 shows the transfer learning performance: (1) Com-\\npared to public available models, such as NASNet-A (Zoph\\net al., 2018) and Inception-v4 (Szegedy et al., 2017), our Ef-\\nﬁcientNet models achieve better accuracy with 4.7x average\\n(up to 21x) parameter reduction. (2) Compared to state-\\nof-the-art models, including DAT (Ngiam et al., 2018) that\\ndynamically synthesizes training data and GPipe (Huang\\net al., 2018) that is trained with specialized pipeline paral-\\nlelism, our EfﬁcientNet models still surpass their accuracy\\nin 5 out of 8 datasets, but using 9.6x fewer parameters\\nFigure 6 compares the accuracy-parameters curve for a va-\\nriety of models. In general, our EfﬁcientNets consistently\\nachieve better accuracy with an order of magnitude fewer pa-\\nrameters than existing models, including ResNet (He et al.,\\n2016), DenseNet (Huang et al., 2017), Inception (Szegedy\\net al., 2017), and NASNet (Zoph et al., 2018).\\n6. Discussion\\nTo disentangle the contribution of our proposed scaling\\nmethod from the EfﬁcientNet architecture, Figure 8 com-\\npares the ImageNet performance of different scaling meth-\\n0 1 2 3 4 5\\nFLOPS (Billions)757677787980818283ImageNet Top-1 Accuracy(%)\\nscale by width\\nscale by depth\\nscale by resolution\\ncompound scalingFigure 8. Scaling Up EfﬁcientNet-B0 with Different Methods.\\nTable 7. Scaled Models Used in Figure 7.\\nModel FLOPS Top-1 Acc.\\nBaseline model (EfﬁcientNet-B0) 0.4B 77.3%\\nScale model by depth ( d=4) 1.8B 79.0%\\nScale model by width ( w=2) 1.8B 78.9%\\nScale model by resolution ( r=2) 1.9B 79.1%\\nCompound Scale ( ddd=1.4,www=1.2,rrr=1.3) 1.8B 81.1%\\nods for the same EfﬁcientNet-B0 baseline network. In gen-\\neral, all scaling methods improve accuracy with the cost\\nof more FLOPS, but our compound scaling method can\\nfurther improve accuracy, by up to 2.5%, than other single-\\ndimension scaling methods, suggesting the importance of\\nour proposed compound scaling.\\nIn order to further understand why our compound scaling\\nmethod is better than others, Figure 7 compares the class\\nactivation map (Zhou et al., 2016) for a few representative\\nmodels with different scaling methods. All these models are\\nscaled from the same baseline, and their statistics are shown\\nin Table 7. Images are randomly picked from ImageNet\\nvalidation set. As shown in the ﬁgure, the model with com-\\npound scaling tends to focus on more relevant regions with\\nmore object details, while other models are either lack of\\nobject details or unable to capture all objects in the images.', metadata={'source': 'C:\\\\Users\\\\bibas\\\\Downloads\\\\EfficientNet- Rethinking Model Scaling for Convolutional Neural Networks.pdf', 'page': 7}),\n",
       " Document(page_content='EfﬁcientNet: Rethinking Model Scaling for Convolutional Neural Networks\\n7. Conclusion\\nIn this paper, we systematically study ConvNet scaling and\\nidentify that carefully balancing network width, depth, and\\nresolution is an important but missing piece, preventing us\\nfrom better accuracy and efﬁciency. To address this issue,\\nwe propose a simple and highly effective compound scaling\\nmethod, which enables us to easily scale up a baseline Con-\\nvNet to any target resource constraints in a more principled\\nway, while maintaining model efﬁciency. Powered by this\\ncompound scaling method, we demonstrate that a mobile-\\nsize EfﬁcientNet model can be scaled up very effectively,\\nsurpassing state-of-the-art accuracy with an order of magni-\\ntude fewer parameters and FLOPS, on both ImageNet and\\nﬁve commonly used transfer learning datasets.\\nAcknowledgements\\nWe thank Ruoming Pang, Vijay Vasudevan, Alok Aggarwal,\\nBarret Zoph, Hongkun Yu, Jonathon Shlens, Raphael Gon-\\ntijo Lopes, Yifeng Lu, Daiyi Peng, Xiaodan Song, Samy\\nBengio, Jeff Dean, and the Google Brain team for their help.\\nAppendix\\nSince 2017, most research papers only report and compare\\nImageNet validation accuracy; this paper also follows this\\nconvention for better comparison. In addition, we have\\nalso veriﬁed the test accuracy by submitting our predictions\\non the 100k test set images to http://image-net.org ;\\nresults are in Table 8. As expected, the test accuracy is very\\nclose to the validation accuracy.\\nTable 8. ImageNet Validation vs. Test Top-1/5 Accuracy .\\nB0 B1 B2 B3 B4 B5 B6 B7\\nVal top1 77.11 79.13 80.07 81.59 82.89 83.60 83.95 84.26\\nTest top1 77.23 79.17 80.16 81.72 82.94 83.69 84.04 84.33\\nVal top5 93.35 94.47 94.90 95.67 96.37 96.71 96.76 96.97\\nTest top5 93.45 94.43 94.98 95.70 96.27 96.64 96.86 96.94\\nReferences\\nBerg, T., Liu, J., Woo Lee, S., Alexander, M. L., Jacobs,\\nD. W., and Belhumeur, P. N. Birdsnap: Large-scale\\nﬁne-grained visual categorization of birds. CVPR , pp.\\n2011–2018, 2014.\\nBossard, L., Guillaumin, M., and Van Gool, L. Food-101–\\nmining discriminative components with random forests.\\nECCV , pp. 446–461, 2014.\\nCai, H., Zhu, L., and Han, S. Proxylessnas: Direct neural\\narchitecture search on target task and hardware. ICLR ,\\n2019.\\nChollet, F. Xception: Deep learning with depthwise separa-\\nble convolutions. CVPR , pp. 1610–02357, 2017.Cubuk, E. D., Zoph, B., Mane, D., Vasudevan, V ., and Le,\\nQ. V . Autoaugment: Learning augmentation policies\\nfrom data. CVPR , 2019.\\nElfwing, S., Uchibe, E., and Doya, K. Sigmoid-weighted\\nlinear units for neural network function approximation\\nin reinforcement learning. Neural Networks , 107:3–11,\\n2018.\\nGholami, A., Kwon, K., Wu, B., Tai, Z., Yue, X., Jin, P.,\\nZhao, S., and Keutzer, K. Squeezenext: Hardware-aware\\nneural network design. ECV Workshop at CVPR’18 ,\\n2018.\\nHan, S., Mao, H., and Dally, W. J. Deep compression:\\nCompressing deep neural networks with pruning, trained\\nquantization and huffman coding. ICLR , 2016.\\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual\\nlearning for image recognition. CVPR , pp. 770–778,\\n2016.\\nHe, K., Gkioxari, G., Doll ´ar, P., and Girshick, R. Mask\\nr-cnn. ICCV , pp. 2980–2988, 2017.\\nHe, Y ., Lin, J., Liu, Z., Wang, H., Li, L.-J., and Han, S.\\nAmc: Automl for model compression and acceleration\\non mobile devices. ECCV , 2018.\\nHendrycks, D. and Gimpel, K. Gaussian error linear units\\n(gelus). arXiv preprint arXiv:1606.08415 , 2016.\\nHoward, A. G., Zhu, M., Chen, B., Kalenichenko, D., Wang,\\nW., Weyand, T., Andreetto, M., and Adam, H. Mobilenets:\\nEfﬁcient convolutional neural networks for mobile vision\\napplications. arXiv preprint arXiv:1704.04861 , 2017.\\nHu, J., Shen, L., and Sun, G. Squeeze-and-excitation net-\\nworks. CVPR , 2018.\\nHuang, G., Sun, Y ., Liu, Z., Sedra, D., and Weinberger,\\nK. Q. Deep networks with stochastic depth. ECCV , pp.\\n646–661, 2016.\\nHuang, G., Liu, Z., Van Der Maaten, L., and Weinberger,\\nK. Q. Densely connected convolutional networks. CVPR ,\\n2017.', metadata={'source': 'C:\\\\Users\\\\bibas\\\\Downloads\\\\EfficientNet- Rethinking Model Scaling for Convolutional Neural Networks.pdf', 'page': 8}),\n",
       " Document(page_content='K. Q. Deep networks with stochastic depth. ECCV , pp.\\n646–661, 2016.\\nHuang, G., Liu, Z., Van Der Maaten, L., and Weinberger,\\nK. Q. Densely connected convolutional networks. CVPR ,\\n2017.\\nHuang, Y ., Cheng, Y ., Chen, D., Lee, H., Ngiam, J., Le,\\nQ. V ., and Chen, Z. Gpipe: Efﬁcient training of giant\\nneural networks using pipeline parallelism. arXiv preprint\\narXiv:1808.07233 , 2018.\\nIandola, F. N., Han, S., Moskewicz, M. W., Ashraf, K.,\\nDally, W. J., and Keutzer, K. Squeezenet: Alexnet-level\\naccuracy with 50x fewer parameters and <0.5 mb model\\nsize. arXiv preprint arXiv:1602.07360 , 2016.', metadata={'source': 'C:\\\\Users\\\\bibas\\\\Downloads\\\\EfficientNet- Rethinking Model Scaling for Convolutional Neural Networks.pdf', 'page': 8}),\n",
       " Document(page_content='EfﬁcientNet: Rethinking Model Scaling for Convolutional Neural Networks\\nIoffe, S. and Szegedy, C. Batch normalization: Accelerating\\ndeep network training by reducing internal covariate shift.\\nICML , pp. 448–456, 2015.\\nKornblith, S., Shlens, J., and Le, Q. V . Do better imagenet\\nmodels transfer better? CVPR , 2019.\\nKrause, J., Deng, J., Stark, M., and Fei-Fei, L. Collecting a\\nlarge-scale dataset of ﬁne-grained cars. Second Workshop\\non Fine-Grained Visual Categorizatio , 2013.\\nKrizhevsky, A. and Hinton, G. Learning multiple layers of\\nfeatures from tiny images. Technical Report , 2009.\\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet\\nclassiﬁcation with deep convolutional neural networks.\\nInNIPS , pp. 1097–1105, 2012.\\nLin, H. and Jegelka, S. Resnet with one-neuron hidden\\nlayers is a universal approximator. NeurIPS , pp. 6172–\\n6181, 2018.\\nLin, T.-Y ., Doll ´ar, P., Girshick, R., He, K., Hariharan, B.,\\nand Belongie, S. Feature pyramid networks for object\\ndetection. CVPR , 2017.\\nLiu, C., Zoph, B., Shlens, J., Hua, W., Li, L.-J., Fei-Fei, L.,\\nYuille, A., Huang, J., and Murphy, K. Progressive neural\\narchitecture search. ECCV , 2018.\\nLu, Z., Pu, H., Wang, F., Hu, Z., and Wang, L. The expres-\\nsive power of neural networks: A view from the width.\\nNeurIPS , 2018.\\nMa, N., Zhang, X., Zheng, H.-T., and Sun, J. Shufﬂenet v2:\\nPractical guidelines for efﬁcient cnn architecture design.\\nECCV , 2018.\\nMahajan, D., Girshick, R., Ramanathan, V ., He, K., Paluri,\\nM., Li, Y ., Bharambe, A., and van der Maaten, L. Explor-\\ning the limits of weakly supervised pretraining. arXiv\\npreprint arXiv:1805.00932 , 2018.\\nMaji, S., Rahtu, E., Kannala, J., Blaschko, M., and Vedaldi,\\nA. Fine-grained visual classiﬁcation of aircraft. arXiv\\npreprint arXiv:1306.5151 , 2013.\\nNgiam, J., Peng, D., Vasudevan, V ., Kornblith, S., Le, Q. V .,\\nand Pang, R. Domain adaptive transfer learning with spe-\\ncialist models. arXiv preprint arXiv:1811.07056 , 2018.\\nNilsback, M.-E. and Zisserman, A. Automated ﬂower clas-\\nsiﬁcation over a large number of classes. ICVGIP , pp.\\n722–729, 2008.\\nParkhi, O. M., Vedaldi, A., Zisserman, A., and Jawahar, C.\\nCats and dogs. CVPR , pp. 3498–3505, 2012.Raghu, M., Poole, B., Kleinberg, J., Ganguli, S., and Sohl-\\nDickstein, J. On the expressive power of deep neural\\nnetworks. ICML , 2017.\\nRamachandran, P., Zoph, B., and Le, Q. V . Searching for\\nactivation functions. arXiv preprint arXiv:1710.05941 ,\\n2018.\\nReal, E., Aggarwal, A., Huang, Y ., and Le, Q. V . Regu-\\nlarized evolution for image classiﬁer architecture search.\\nAAAI , 2019.\\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S.,\\nMa, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein,\\nM., et al. Imagenet large scale visual recognition chal-\\nlenge. International Journal of Computer Vision , 115(3):\\n211–252, 2015.\\nSandler, M., Howard, A., Zhu, M., Zhmoginov, A., and\\nChen, L.-C. Mobilenetv2: Inverted residuals and linear\\nbottlenecks. CVPR , 2018.\\nSharir, O. and Shashua, A. On the expressive power of\\noverlapping architectures of deep learning. ICLR , 2018.\\nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I.,\\nand Salakhutdinov, R. Dropout: a simple way to prevent\\nneural networks from overﬁtting. The Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\nSzegedy, C., Liu, W., Jia, Y ., Sermanet, P., Reed, S.,\\nAnguelov, D., Erhan, D., Vanhoucke, V ., and Rabinovich,\\nA. Going deeper with convolutions. CVPR , pp. 1–9,\\n2015.\\nSzegedy, C., Vanhoucke, V ., Ioffe, S., Shlens, J., and Wojna,\\nZ. Rethinking the inception architecture for computer\\nvision. CVPR , pp. 2818–2826, 2016.\\nSzegedy, C., Ioffe, S., Vanhoucke, V ., and Alemi, A. A.\\nInception-v4, inception-resnet and the impact of residual\\nconnections on learning. AAAI , 4:12, 2017.\\nTan, M., Chen, B., Pang, R., Vasudevan, V ., Sandler, M.,\\nHoward, A., and Le, Q. V . MnasNet: Platform-aware\\nneural architecture search for mobile. CVPR , 2019.\\nXie, S., Girshick, R., Doll ´ar, P., Tu, Z., and He, K. Aggre-', metadata={'source': 'C:\\\\Users\\\\bibas\\\\Downloads\\\\EfficientNet- Rethinking Model Scaling for Convolutional Neural Networks.pdf', 'page': 9}),\n",
       " Document(page_content='Howard, A., and Le, Q. V . MnasNet: Platform-aware\\nneural architecture search for mobile. CVPR , 2019.\\nXie, S., Girshick, R., Doll ´ar, P., Tu, Z., and He, K. Aggre-\\ngated residual transformations for deep neural networks.\\nCVPR , pp. 5987–5995, 2017.\\nYang, T.-J., Howard, A., Chen, B., Zhang, X., Go, A., Sze,\\nV ., and Adam, H. Netadapt: Platform-aware neural net-\\nwork adaptation for mobile applications. ECCV , 2018.\\nZagoruyko, S. and Komodakis, N. Wide residual networks.\\nBMVC , 2016.', metadata={'source': 'C:\\\\Users\\\\bibas\\\\Downloads\\\\EfficientNet- Rethinking Model Scaling for Convolutional Neural Networks.pdf', 'page': 9}),\n",
       " Document(page_content='EfﬁcientNet: Rethinking Model Scaling for Convolutional Neural Networks\\nZhang, X., Li, Z., Loy, C. C., and Lin, D. Polynet: A pursuit\\nof structural diversity in very deep networks. CVPR , pp.\\n3900–3908, 2017.\\nZhang, X., Zhou, X., Lin, M., and Sun, J. Shufﬂenet: An ex-\\ntremely efﬁcient convolutional neural network for mobile\\ndevices. CVPR , 2018.\\nZhou, B., Khosla, A., Lapedriza, A., Oliva, A., and Torralba,\\nA. Learning deep features for discriminative localization.\\nCVPR , pp. 2921–2929, 2016.\\nZoph, B. and Le, Q. V . Neural architecture search with\\nreinforcement learning. ICLR , 2017.\\nZoph, B., Vasudevan, V ., Shlens, J., and Le, Q. V . Learning\\ntransferable architectures for scalable image recognition.\\nCVPR , 2018.', metadata={'source': 'C:\\\\Users\\\\bibas\\\\Downloads\\\\EfficientNet- Rethinking Model Scaling for Convolutional Neural Networks.pdf', 'page': 10})]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec3747e-5ca9-4f77-b631-d63d1cce3bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
